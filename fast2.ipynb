{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Fast Scene Understanding.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3wrqIinejhkt","colab_type":"code","colab":{}},"source":["import zipfile as zf\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.optim.lr_scheduler import StepLR\n","import cv2\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","from PIL import Image\n","\n","import future\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1bNQ-SAqjhky","colab_type":"text"},"source":["## 1 - Load Dataset"]},{"cell_type":"code","metadata":{"id":"M24VBWq_jhkz","colab_type":"code","colab":{}},"source":["#!wget https://www.dropbox.com/s/pxcz2wdz04zxocq/CamVid.zip?dl=1 -O CamVid.zip\n","#!unzip CamVid.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdA2ZHq4jhk2","colab_type":"code","colab":{}},"source":["#!unzip CamVid.zip -d images"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o-dj3wA0jhk5","colab_type":"text"},"source":["## 2 - Create the ENet model\n","Major ENet code is borrowed from iArunava/ENet-Real-Time-Semantic-Segmentation\n","We decided to to split the model to three sub classes:\n","\n","1) Initial block\n","\n","2) RDDNeck - class for regular, downsampling and dilated bottlenecks\n","\n","3) ASNeck - class for asymetric bottlenecks\n","\n","4) UBNeck - class for upsampling bottlenecks"]},{"cell_type":"code","metadata":{"id":"SJ2tXkJVjhk6","colab_type":"code","colab":{}},"source":["class InitialBlock(nn.Module):\n","  \n","  # Initial block of the model:\n","  #         Input\n","  #        /     \\\n","  #       /       \\\n","  #maxpool2d    conv2d-3x3\n","  #       \\       /  \n","  #        \\     /\n","  #      concatenate\n","   \n","    def __init__ (self,in_channels = 3,out_channels = 13):\n","        super().__init__()\n","\n","\n","        self.maxpool = nn.MaxPool2d(kernel_size=2, \n","                                      stride = 2, \n","                                      padding = 0)\n","\n","        self.conv = nn.Conv2d(in_channels, \n","                                out_channels,\n","                                kernel_size = 3,\n","                                stride = 2, \n","                                padding = 1)\n","\n","        self.prelu = nn.PReLU(16)\n","\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","  \n","    def forward(self, x):\n","        \n","        main = self.conv(x)\n","        main = self.batchnorm(main)\n","        \n","        side = self.maxpool(x)\n","        \n","        # concatenating on the channels axis\n","        x = torch.cat((main, side), dim=1)\n","        x = self.prelu(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHQIix9Pjhk8","colab_type":"code","colab":{}},"source":["class RDDNeck(nn.Module):\n","    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n","      \n","  # Regular|Dilated|Downsampling bottlenecks:\n","  #\n","  #     Bottleneck Input\n","  #        /        \\\n","  #       /          \\\n","  # maxpooling2d   conv2d-1x1\n","  #      |             | PReLU\n","  #      |         conv2d-3x3\n","  #      |             | PReLU\n","  #      |         conv2d-1x1\n","  #      |             |\n","  #  Padding2d     Regularizer\n","  #       \\           /  \n","  #        \\         /\n","  #      Summing + PReLU\n","  #\n","  # Params: \n","  #  dilation (bool) - if True: creating dilation bottleneck\n","  #  down_flag (bool) - if True: creating downsampling bottleneck\n","  #  projection_ratio - ratio between input and output channels\n","  #  relu - if True: relu used as the activation function else: Prelu us used\n","  #  p - dropout ratio\n","        \n","        super().__init__()\n","        \n","        # Define class variables\n","        self.in_channels = in_channels\n","        \n","        self.out_channels = out_channels\n","        self.dilation = dilation\n","        self.down_flag = down_flag\n","        \n","        # calculating the number of reduced channels\n","        if down_flag:\n","            self.stride = 2\n","            self.reduced_depth = int(in_channels // projection_ratio)\n","        else:\n","            self.stride = 1\n","            self.reduced_depth = int(out_channels // projection_ratio)\n","        \n","        if relu:\n","            activation = nn.ReLU()\n","        else:\n","            activation = nn.PReLU()\n","        \n","        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n","                                      stride = 2,\n","                                      padding = 0, return_indices=True)\n","        \n","\n","        \n","        self.dropout = nn.Dropout2d(p=p)\n","\n","        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n","                               out_channels = self.reduced_depth,\n","                               kernel_size = 1,\n","                               stride = 1,\n","                               padding = 0,\n","                               bias = False,\n","                               dilation = 1)\n","        \n","        self.prelu1 = activation\n","        \n","        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n","                                  out_channels = self.reduced_depth,\n","                                  kernel_size = 3,\n","                                  stride = self.stride,\n","                                  padding = self.dilation,\n","                                  bias = True,\n","                                  dilation = self.dilation)\n","                                  \n","        self.prelu2 = activation\n","        \n","        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n","                                  out_channels = self.out_channels,\n","                                  kernel_size = 1,\n","                                  stride = 1,\n","                                  padding = 0,\n","                                  bias = False,\n","                                  dilation = 1)\n","        \n","        self.prelu3 = activation\n","        \n","        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n","        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n","        \n","        \n","    def forward(self, x):\n","        \n","        bs = x.size()[0]\n","        x_copy = x\n","        \n","        # Side Branch\n","        x = self.conv1(x)\n","        x = self.batchnorm(x)\n","        x = self.prelu1(x)\n","        \n","        x = self.conv2(x)\n","        x = self.batchnorm(x)\n","        x = self.prelu2(x)\n","        \n","        x = self.conv3(x)\n","        x = self.batchnorm2(x)\n","                \n","        x = self.dropout(x)\n","        \n","        # Main Branch\n","        if self.down_flag:\n","            x_copy, indices = self.maxpool(x_copy)\n","          \n","        if self.in_channels != self.out_channels:\n","            out_shape = self.out_channels - self.in_channels\n","            \n","            #padding and concatenating in order to match the channels axis of the side and main branches\n","            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n","            if torch.cuda.is_available():\n","                extras = extras.cuda()\n","            x_copy = torch.cat((x_copy, extras), dim = 1)\n","\n","        # Summing main and side branches\n","        x = x + x_copy\n","        x = self.prelu3(x)\n","        \n","        if self.down_flag:\n","            return x, indices\n","        else:\n","            return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KPH7yKPjhk_","colab_type":"code","colab":{}},"source":["class ASNeck(nn.Module):\n","    def __init__(self, in_channels, out_channels, projection_ratio=4):\n","      \n","  # Asymetric bottleneck:\n","  #\n","  #     Bottleneck Input\n","  #        /        \\\n","  #       /          \\\n","  #      |         conv2d-1x1\n","  #      |             | PReLU\n","  #      |         conv2d-1x5\n","  #      |             |\n","  #      |         conv2d-5x1\n","  #      |             | PReLU\n","  #      |         conv2d-1x1\n","  #      |             |\n","  #  Padding2d     Regularizer\n","  #       \\           /  \n","  #        \\         /\n","  #      Summing + PReLU\n","  #\n","  # Params:    \n","  #  projection_ratio - ratio between input and output channels\n","        \n","        super().__init__()\n","        \n","        # Define class variables\n","        self.in_channels = in_channels\n","        self.reduced_depth = int(in_channels / projection_ratio)\n","        self.out_channels = out_channels\n","        \n","        self.dropout = nn.Dropout2d(p=0.1)\n","        \n","        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n","                               out_channels = self.reduced_depth,\n","                               kernel_size = 1,\n","                               stride = 1,\n","                               padding = 0,\n","                               bias = False)\n","        \n","        self.prelu1 = nn.PReLU()\n","        \n","        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n","                                  out_channels = self.reduced_depth,\n","                                  kernel_size = (1, 5),\n","                                  stride = 1,\n","                                  padding = (0, 2),\n","                                  bias = False)\n","        \n","        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n","                                  out_channels = self.reduced_depth,\n","                                  kernel_size = (5, 1),\n","                                  stride = 1,\n","                                  padding = (2, 0),\n","                                  bias = False)\n","        \n","        self.prelu2 = nn.PReLU()\n","        \n","        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n","                                  out_channels = self.out_channels,\n","                                  kernel_size = 1,\n","                                  stride = 1,\n","                                  padding = 0,\n","                                  bias = False)\n","        \n","        self.prelu3 = nn.PReLU()\n","        \n","        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n","        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n","        \n","    def forward(self, x):\n","        bs = x.size()[0]\n","        x_copy = x\n","        \n","        # Side Branch\n","        x = self.conv1(x)\n","        x = self.batchnorm(x)\n","        x = self.prelu1(x)\n","        \n","        x = self.conv21(x)\n","        x = self.conv22(x)\n","        x = self.batchnorm(x)\n","        x = self.prelu2(x)\n","        \n","        x = self.conv3(x)\n","                \n","        x = self.dropout(x)\n","        x = self.batchnorm2(x)\n","        \n","        # Main Branch\n","        \n","        if self.in_channels != self.out_channels:\n","            out_shape = self.out_channels - self.in_channels\n","            \n","            #padding and concatenating in order to match the channels axis of the side and main branches\n","            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n","            if torch.cuda.is_available():\n","                extras = extras.cuda()\n","            x_copy = torch.cat((x_copy, extras), dim = 1)\n","        \n","        # Summing main and side branches\n","        x = x + x_copy\n","        x = self.prelu3(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UXbz-4SOjhlC","colab_type":"code","colab":{}},"source":["class UBNeck(nn.Module):\n","    \n","  # Upsampling bottleneck:\n","  #     Bottleneck Input\n","  #        /        \\\n","  #       /          \\\n","  # conv2d-1x1     convTrans2d-1x1\n","  #      |             | PReLU\n","  #      |         convTrans2d-3x3\n","  #      |             | PReLU\n","  #      |         convTrans2d-1x1\n","  #      |             |\n","  # maxunpool2d    Regularizer\n","  #       \\           /  \n","  #        \\         /\n","  #      Summing + PReLU\n","  #\n","  #  Params: \n","  #  projection_ratio - ratio between input and output channels\n","  #  relu - if True: relu used as the activation function else: Prelu us used\n","  \n","    def __init__(self, in_channels, out_channels, relu=False, projection_ratio=4):\n","        \n","        super().__init__()\n","        \n","        # Define class variables\n","        self.in_channels = in_channels\n","        self.reduced_depth = int(in_channels / projection_ratio)\n","        self.out_channels = out_channels\n","        \n","        \n","        if relu:\n","            activation = nn.ReLU()\n","        else:\n","            activation = nn.PReLU()\n","        \n","        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n","                                     stride = 2)\n","        \n","        self.main_conv = nn.Conv2d(in_channels = self.in_channels,\n","                                    out_channels = self.out_channels,\n","                                    kernel_size = 1)\n","        \n","        self.dropout = nn.Dropout2d(p=0.1)\n","        \n","        \n","        self.convt1 = nn.ConvTranspose2d(in_channels = self.in_channels,\n","                               out_channels = self.reduced_depth,\n","                               kernel_size = 1,\n","                               padding = 0,\n","                               bias = False)\n","        \n","        \n","        self.prelu1 = activation\n","        \n","        # This layer used for Upsampling\n","        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n","                                  out_channels = self.reduced_depth,\n","                                  kernel_size = 3,\n","                                  stride = 2,\n","                                  padding = 1,\n","                                  output_padding = 1,\n","                                  bias = False)\n","        \n","        self.prelu2 = activation\n","        \n","        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n","                                  out_channels = self.out_channels,\n","                                  kernel_size = 1,\n","                                  padding = 0,\n","                                  bias = False)\n","        \n","        self.prelu3 = activation\n","        \n","        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n","        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n","        \n","    def forward(self, x, indices):\n","        x_copy = x\n","        \n","        # Side Branch\n","        x = self.convt1(x)\n","        x = self.batchnorm(x)\n","        x = self.prelu1(x)\n","        \n","        x = self.convt2(x)\n","        x = self.batchnorm(x)\n","        x = self.prelu2(x)\n","        \n","        x = self.convt3(x)\n","        x = self.batchnorm2(x)\n","        \n","        x = self.dropout(x)\n","        \n","        # Main Branch\n","        \n","        x_copy = self.main_conv(x_copy)\n","        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n","        \n","        # summing the main and side branches\n","        x = x + x_copy\n","        x = self.prelu3(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrFQWeggjhlH","colab_type":"code","colab":{}},"source":["class ENet(nn.Module):\n","  \n","  # Creating Enet model!\n","  \n","    def __init__(self, C):\n","        super().__init__()\n","        \n","        # Define class variables\n","        # C - number of classes\n","        self.C = C\n","        \n","        # The initial block\n","        self.init = InitialBlock()\n","        \n","        \n","        # The first bottleneck\n","        self.b10 = RDDNeck(dilation=1, \n","                           in_channels=16, \n","                           out_channels=64, \n","                           down_flag=True, \n","                           p=0.01)\n","        \n","        self.b11 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           p=0.01)\n","        \n","        self.b12 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           p=0.01)\n","        \n","        self.b13 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           p=0.01)\n","        \n","        self.b14 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           p=0.01)\n","        \n","        \n","        # The second bottleneck\n","        self.b20 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=128, \n","                           down_flag=True)\n","        \n","        self.b21 = RDDNeck(dilation=1, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b22 = RDDNeck(dilation=2, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b23 = ASNeck(in_channels=128, \n","                          out_channels=128)\n","        \n","        self.b24 = RDDNeck(dilation=4, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b25 = RDDNeck(dilation=1, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b26 = RDDNeck(dilation=8, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b27 = ASNeck(in_channels=128, \n","                          out_channels=128)\n","        \n","        self.b28 = RDDNeck(dilation=16, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        \n","        # The third bottleneck\n","        self.b31 = RDDNeck(dilation=1, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b32 = RDDNeck(dilation=2, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b33 = ASNeck(in_channels=128, \n","                          out_channels=128)\n","        \n","        self.b34 = RDDNeck(dilation=4, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b35 = RDDNeck(dilation=1, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b36 = RDDNeck(dilation=8, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b37 = ASNeck(in_channels=128, \n","                          out_channels=128)\n","        \n","        self.b38 = RDDNeck(dilation=16, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        \n","        # The fourth bottleneck\n","        self.b40 = UBNeck(in_channels=128, \n","                          out_channels=64, \n","                          relu=True)\n","        \n","        self.b41 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           relu=True)\n","        \n","        self.b42 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           relu=True)\n","        \n","        \n","        # The fifth bottleneck\n","        self.b50 = UBNeck(in_channels=64, \n","                          out_channels=16, \n","                          relu=True)\n","        \n","        self.b51 = RDDNeck(dilation=1, \n","                           in_channels=16, \n","                           out_channels=16, \n","                           down_flag=False, \n","                           relu=True)\n","        \n","        \n","        # Final ConvTranspose Layer\n","        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n","                                           out_channels=self.C, \n","                                           kernel_size=3, \n","                                           stride=2, \n","                                           padding=1, \n","                                           output_padding=1,\n","                                           bias=False)\n","        \n","        \n","    def forward(self, x):\n","        \n","        # The initial block\n","        x = self.init(x)\n","        \n","        # The first bottleneck\n","        x, i1 = self.b10(x)\n","        x = self.b11(x)\n","        x = self.b12(x)\n","        x = self.b13(x)\n","        x = self.b14(x)\n","        \n","        # The second bottleneck\n","        x, i2 = self.b20(x)\n","        x = self.b21(x)\n","        x = self.b22(x)\n","        x = self.b23(x)\n","        x = self.b24(x)\n","        x = self.b25(x)\n","        x = self.b26(x)\n","        x = self.b27(x)\n","        x = self.b28(x)\n","\n","        # The third bottleneck\n","        x = self.b31(x)\n","        x = self.b32(x)\n","        x = self.b33(x)\n","        x = self.b34(x)\n","        x = self.b35(x)\n","        x = self.b36(x)\n","        x = self.b37(x)\n","        x = self.b38(x)\n","        \n","        # The fourth bottleneck\n","        x = self.b40(x, i2)\n","        x = self.b41(x)\n","        x = self.b42(x)\n","        \n","        # The fifth bottleneck\n","        x = self.b50(x, i1)\n","        x = self.b51(x)\n","        \n","        # Final ConvTranspose Layer\n","        x = self.fullconv(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeDTxGHDjhlK","colab_type":"code","colab":{}},"source":["class ENetDecoder(nn.Module):\n","    # Creating Branched Enet model!\n","  \n","    def __init__(self, C):\n","        super().__init__()\n","        \n","        # Define class variables\n","        # C - number of classes\n","        self.C = C    \n","        \n","         \n","        # The third bottleneck\n","        self.b31 = RDDNeck(dilation=1, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b32 = RDDNeck(dilation=2, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b33 = ASNeck(in_channels=128, \n","                          out_channels=128)\n","        \n","        self.b34 = RDDNeck(dilation=4, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b35 = RDDNeck(dilation=1, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b36 = RDDNeck(dilation=8, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b37 = ASNeck(in_channels=128, \n","                          out_channels=128)\n","        \n","        self.b38 = RDDNeck(dilation=16, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        \n","        # The fourth bottleneck\n","        self.b40 = UBNeck(in_channels=128, \n","                          out_channels=64, \n","                          relu=True)\n","        \n","        self.b41 = RDDNeck(dilation=1, \n","                           in_channels=64, #originally 64\n","                           out_channels=64, \n","                           down_flag=False, \n","                           relu=True)\n","        \n","        self.b42 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           relu=True)\n","        \n","        \n","        # The fifth bottleneck\n","        self.b50 = UBNeck(in_channels=64, \n","                          out_channels=16, \n","                          relu=True)\n","        \n","        self.b51 = RDDNeck(dilation=1, \n","                           in_channels=16, # originally 16\n","                           out_channels=16, \n","                           down_flag=False, \n","                           relu=True)\n","        \n","        \n","        # Final ConvTranspose Layer\n","        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n","                                           out_channels=self.C, \n","                                           kernel_size=3, \n","                                           stride=2, \n","                                           padding=1, \n","                                           output_padding=1,\n","                                           bias=False)\n","        \n","        \n","    def forward(self, x, i1, i2):\n","        \n","        # The third bottleneck\n","        x = self.b31(x)\n","        x = self.b32(x)\n","        x = self.b33(x)\n","        x = self.b34(x)\n","        x = self.b35(x)\n","        x = self.b36(x)\n","        x = self.b37(x)\n","        x = self.b38(x)\n","        \n","        # The fourth bottleneck\n","        x = self.b40(x, i2)\n","        x = self.b41(x)\n","        x = self.b42(x)\n","        \n","        # The fifth bottleneck\n","        x = self.b50(x, i1)\n","        x = self.b51(x)\n","        \n","        # Final ConvTranspose Layer\n","        x = self.fullconv(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ue6fHx87jhlN","colab_type":"code","colab":{}},"source":["class BranchedModule(nn.Module):\n","    def __init__(self, C):\n","        super(BranchedModule, self).__init__()\n","        self.C = C\n","        self.layer1 = ENetDecoder(C)\n","        self.layer2 = ENetDecoder(C)\n","        self.layer3 = ENetDecoder(C)\n","        \n","    def forward(self,x,i1,i2):\n","        y = [self.layer1(x,i1,i2), self.layer2(x,i1,i2), self.layer3(x,i1,i2)]\n","        return y\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJIZKwVPjhlQ","colab_type":"code","colab":{}},"source":["class BranchedENet(nn.Module):\n","    def __init__(self, C):\n","        super().__init__()\n","\n","        # Define class variables\n","        # C - number of classes\n","        self.C = C\n","        \n","        # The initial block\n","        self.init = InitialBlock()\n","        \n","        \n","        # The first bottleneck\n","        self.b10 = RDDNeck(dilation=1, \n","                           in_channels=16, \n","                           out_channels=64, \n","                           down_flag=True, \n","                           p=0.01)\n","        \n","        self.b11 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           p=0.01)\n","        \n","        self.b12 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           p=0.01)\n","        \n","        self.b13 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           p=0.01)\n","        \n","        self.b14 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=64, \n","                           down_flag=False, \n","                           p=0.01)\n","        \n","        \n","        # The second bottleneck\n","        self.b20 = RDDNeck(dilation=1, \n","                           in_channels=64, \n","                           out_channels=128, \n","                           down_flag=True)\n","        \n","        self.b21 = RDDNeck(dilation=1, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b22 = RDDNeck(dilation=2, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b23 = ASNeck(in_channels=128, \n","                          out_channels=128)\n","        \n","        self.b24 = RDDNeck(dilation=4, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b25 = RDDNeck(dilation=1, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b26 = RDDNeck(dilation=8, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.b27 = ASNeck(in_channels=128, \n","                          out_channels=128)\n","        \n","        self.b28 = RDDNeck(dilation=16, \n","                           in_channels=128, \n","                           out_channels=128, \n","                           down_flag=False)\n","        \n","        self.branch = BranchedModule(C)\n","        \n","        \n","    def forward(self, x):\n","\n","        # The initial block\n","        x = self.init(x)\n","        \n","        # The first bottleneck\n","        x, i1 = self.b10(x)\n","        x = self.b11(x)\n","        x = self.b12(x)\n","        x = self.b13(x)\n","        x = self.b14(x)\n","\n","         # the second bottleneck\n","        x, i2 = self.b20(x)\n","        x = self.b21(x)\n","        x = self.b22(x)\n","        x = self.b23(x)\n","        x = self.b24(x)\n","        x = self.b25(x)\n","        x = self.b26(x)\n","        x = self.b27(x)\n","        x = self.b28(x)\n","\n","        x = self.branch(x, i1, i2)\n","            \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SoSdRBCAjhlU","colab_type":"code","colab":{}},"source":["#pretrained_model = torch.load('models/ckpt-enet.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gpCP5JKAjhlW","colab_type":"text"},"source":["## 3 - Losses(todo)\n","(1) Semantic Segmentation Loss\n","\n","(2) Instantance Segmentation Loss\n","\n","(3) Depth Estimation Loss"]},{"cell_type":"code","metadata":{"id":"0b6IvyXIjhlW","colab_type":"code","outputId":"fcf90e6c-f62f-4f20-d082-365d2b961e2f","executionInfo":{"status":"ok","timestamp":1586973931065,"user_tz":240,"elapsed":357,"user":{"displayName":"Shashwat Shivam","photoUrl":"","userId":"00567306531573054305"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["\"\"\"\n","def compute_instance_cost(a_C, a_G):\n","    M = {}\n","    tools = require 'tools/tools'\n","\n","    in_margin = 0.5\n","    out_margin = 1.5\n","    Lnorm = 2\n","\n","    function norm(inp, L)\n","        n\n","        if (L == 1) then\n","            n = torch.sum(torch.abs(inp), 1)\n","        else\n","            n = torch.sqrt(torch.sum(torch.pow(inp, 2), 1) + 1e-8)\n","        end\n","        return n\n","    end\n","\n","    -- prediction: batchsize x nDim x h x w\n","    -- labels: batchsize x classes x h x w\n","\n","    local lossf =\n","        function(prediction, labels)\n","        local batchsize = prediction:size(1)\n","        local c = prediction:size(2)\n","        local height = prediction:size(3)\n","        local width = prediction:size(4)\n","        local nInstanceMaps = labels:size(2)\n","        local loss = 0\n","\n","        M.loss_dist = 0\n","        M.loss_var = 0\n","\n","        for b = 1, batchsize do\n","            local pred = prediction[b] -- c x h x w\n","            local loss_var = 0\n","            local loss_dist = 0\n","\n","            for h = 1, nInstanceMaps do\n","                local label = labels[b][h]:view(1, height, width) -- 1 x h x w\n","                local means = {}\n","                local loss_v = 0\n","                local loss_d = 0\n","\n","                -- center pull force\n","                for j = 1, label:max() do\n","                    local mask = label:eq(j)\n","                    local mask_sum = mask:sum()\n","                    if (mask_sum > 1) then\n","                        local inst = pred[mask:expandAs(pred)]:view(c, -1, 1) -- c x -1 x 1\n","\n","                        -- Calculate mean of instance\n","                        local mean = torch.mean(inst, 2) -- c x 1 x 1\n","                        table.insert(means, mean)\n","\n","                        -- Calculate variance of instance\n","                        local var = norm((inst - mean:expandAs(inst)), 2) -- 1 x -1 x 1\n","                        var = torch.cmax(var - (in_margin), 0)\n","                        local not_hinged = torch.sum(torch.gt(var, 0))\n","\n","                        var = torch.pow(var, 2)\n","                        var = var:view(-1)\n","\n","                        var = torch.mean(var)\n","                        loss_v = loss_v + var\n","                    end\n","                end\n","\n","                loss_var = loss_var + loss_v\n","\n","                -- center push force\n","                if (#means > 1) then\n","                    for j = 1, #means do\n","                        local mean_A = means[j] -- c x 1 x 1\n","                        for k = j + 1, #means do\n","                            local mean_B = means[k] -- c x 1 x 1\n","                            local d = norm(mean_A - mean_B, Lnorm) -- 1 x 1 x 1\n","                            d = torch.pow(torch.cmax(-(d - 2 * out_margin), 0), 2)\n","                            loss_d = loss_d + d[1][1][1]\n","                        end\n","                    end\n","\n","                    loss_dist = loss_dist + loss_d / ((#means - 1) + 1e-8)\n","                end\n","            end\n","\n","            loss = loss + (loss_dist + loss_var)\n","        end\n","\n","        loss = loss / batchsize + torch.sum(prediction) * 0\n","\n","        return loss\n","    end\n","\n","return lossf\n","\"\"\""],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef compute_instance_cost(a_C, a_G):\\n    M = {}\\n    tools = require 'tools/tools'\\n\\n    in_margin = 0.5\\n    out_margin = 1.5\\n    Lnorm = 2\\n\\n    function norm(inp, L)\\n        n\\n        if (L == 1) then\\n            n = torch.sum(torch.abs(inp), 1)\\n        else\\n            n = torch.sqrt(torch.sum(torch.pow(inp, 2), 1) + 1e-8)\\n        end\\n        return n\\n    end\\n\\n    -- prediction: batchsize x nDim x h x w\\n    -- labels: batchsize x classes x h x w\\n\\n    local lossf =\\n        function(prediction, labels)\\n        local batchsize = prediction:size(1)\\n        local c = prediction:size(2)\\n        local height = prediction:size(3)\\n        local width = prediction:size(4)\\n        local nInstanceMaps = labels:size(2)\\n        local loss = 0\\n\\n        M.loss_dist = 0\\n        M.loss_var = 0\\n\\n        for b = 1, batchsize do\\n            local pred = prediction[b] -- c x h x w\\n            local loss_var = 0\\n            local loss_dist = 0\\n\\n            for h = 1, nInstanceMaps do\\n                local label = labels[b][h]:view(1, height, width) -- 1 x h x w\\n                local means = {}\\n                local loss_v = 0\\n                local loss_d = 0\\n\\n                -- center pull force\\n                for j = 1, label:max() do\\n                    local mask = label:eq(j)\\n                    local mask_sum = mask:sum()\\n                    if (mask_sum > 1) then\\n                        local inst = pred[mask:expandAs(pred)]:view(c, -1, 1) -- c x -1 x 1\\n\\n                        -- Calculate mean of instance\\n                        local mean = torch.mean(inst, 2) -- c x 1 x 1\\n                        table.insert(means, mean)\\n\\n                        -- Calculate variance of instance\\n                        local var = norm((inst - mean:expandAs(inst)), 2) -- 1 x -1 x 1\\n                        var = torch.cmax(var - (in_margin), 0)\\n                        local not_hinged = torch.sum(torch.gt(var, 0))\\n\\n                        var = torch.pow(var, 2)\\n                        var = var:view(-1)\\n\\n                        var = torch.mean(var)\\n                        loss_v = loss_v + var\\n                    end\\n                end\\n\\n                loss_var = loss_var + loss_v\\n\\n                -- center push force\\n                if (#means > 1) then\\n                    for j = 1, #means do\\n                        local mean_A = means[j] -- c x 1 x 1\\n                        for k = j + 1, #means do\\n                            local mean_B = means[k] -- c x 1 x 1\\n                            local d = norm(mean_A - mean_B, Lnorm) -- 1 x 1 x 1\\n                            d = torch.pow(torch.cmax(-(d - 2 * out_margin), 0), 2)\\n                            loss_d = loss_d + d[1][1][1]\\n                        end\\n                    end\\n\\n                    loss_dist = loss_dist + loss_d / ((#means - 1) + 1e-8)\\n                end\\n            end\\n\\n            loss = loss + (loss_dist + loss_var)\\n        end\\n\\n        loss = loss / batchsize + torch.sum(prediction) * 0\\n\\n        return loss\\n    end\\n\\nreturn lossf\\n\""]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"4Dv3_I_Ljhla","colab_type":"code","outputId":"880d4172-a382-4afa-b067-2f6ce96478ce","executionInfo":{"status":"ok","timestamp":1586973931683,"user_tz":240,"elapsed":182,"user":{"displayName":"Shashwat Shivam","photoUrl":"","userId":"00567306531573054305"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["\"\"\"\n","def compute_huber_loss(input_i, label):\n","    local mask = label:gt(0)\n","    local d = input_i[mask] - label[mask]\n","    local ds = d:size(1)\n","\n","    local da = torch.abs(d)\n","    local d2 = torch.pow(d, 2)\n","\n","    local th = 1 / 5 * torch.max(da)\n","    local mask2 = torch.gt(da, th)\n","    da[mask2] = (d2[mask2] + (th * th)) / (2 * th)\n","\n","    return 1 / ds * torch.sum(da)\n","\n","return loss\n","\"\"\""],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef compute_huber_loss(input_i, label):\\n    local mask = label:gt(0)\\n    local d = input_i[mask] - label[mask]\\n    local ds = d:size(1)\\n\\n    local da = torch.abs(d)\\n    local d2 = torch.pow(d, 2)\\n\\n    local th = 1 / 5 * torch.max(da)\\n    local mask2 = torch.gt(da, th)\\n    da[mask2] = (d2[mask2] + (th * th)) / (2 * th)\\n\\n    return 1 / ds * torch.sum(da)\\n\\nreturn loss\\n'"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"lHm_GLNBjhld","colab_type":"code","outputId":"f68d30bc-3d49-48be-8bbe-f755143efdd1","executionInfo":{"status":"ok","timestamp":1586973932495,"user_tz":240,"elapsed":159,"user":{"displayName":"Shashwat Shivam","photoUrl":"","userId":"00567306531573054305"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["\"\"\"local grad = require 'autograd'\n","\n","def lossfunction(lossf_name, weights):\n","    if (lossf_name == 'softmaxLoss') then\n","        lossfunction = cudnn.SpatialCrossEntropyCriterion(weights)\n","    elseif (lossf_name == 'huberLoss') then\n","        lossfunction = grad.nn.AutoCriterion('depthLoss_huber')(require 'lossf/myHuberLoss')\n","    elseif (lossf_name == 'instanceLoss') then\n","        lossfunction = grad.nn.AutoCriterion('instance_loss')(require 'lossf/myInstanceLoss')\n","    else\n","        assert(false, 'Cannot load lossfunction ' .. opts.lossf)\n","    end\n","\n","    return lossfunction\n","end\n","\n","return getLoss\"\"\""],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"local grad = require 'autograd'\\n\\ndef lossfunction(lossf_name, weights):\\n    if (lossf_name == 'softmaxLoss') then\\n        lossfunction = cudnn.SpatialCrossEntropyCriterion(weights)\\n    elseif (lossf_name == 'huberLoss') then\\n        lossfunction = grad.nn.AutoCriterion('depthLoss_huber')(require 'lossf/myHuberLoss')\\n    elseif (lossf_name == 'instanceLoss') then\\n        lossfunction = grad.nn.AutoCriterion('instance_loss')(require 'lossf/myInstanceLoss')\\n    else\\n        assert(false, 'Cannot load lossfunction ' .. opts.lossf)\\n    end\\n\\n    return lossfunction\\nend\\n\\nreturn getLoss\""]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"XnRAXNvbjhlf","colab_type":"text"},"source":["## 4 - Instantiate the ENet model"]},{"cell_type":"code","metadata":{"id":"5Zo6pIjqjhlg","colab_type":"code","colab":{}},"source":["enet = BranchedENet(12)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQAvwXDpjhlj","colab_type":"code","colab":{}},"source":["# Checking if there is any gpu available and pass the model to gpu or cpu\n","device = torch.device('cuda:0')\n","#device = torch.device('cpu')\n","enet = enet.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8nbWbnYt-WNT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":280},"outputId":"053d3cf9-8019-4c8b-9bcf-e5877764beac","executionInfo":{"status":"error","timestamp":1586974788263,"user_tz":240,"elapsed":2048,"user":{"displayName":"Shashwat Shivam","photoUrl":"","userId":"00567306531573054305"}}},"source":["from google.colab import drive\n","drive.mount('Colab_Notebooks')\n","\n","import sys\n","sys.path.append('Colab_Notebooks')\n","\n","print(os.listdir())\n","from cityscapes import Cityscapes as dataset"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Mounted at Colab_Notebooks\n","['.config', 'Colab_Notebooks', 'sample_data']\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-dcce320ed693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcityscapes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCityscapes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_fill_cache\u001b[0;34m(self)\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected: '/Colab_Notebooks'"]}]},{"cell_type":"code","metadata":{"id":"2Ue8_cXYEhGj","colab_type":"code","colab":{}},"source":["import os\n","from PIL import Image\n","import numpy as np\n","\n","\n","def get_files(folder, name_filter=None, extension_filter=None):\n","    \"\"\"Helper function that returns the list of files in a specified folder\n","    with a specified extension.\n","    Keyword arguments:\n","    - folder (``string``): The path to a folder.\n","    - name_filter (```string``, optional): The returned files must contain\n","    this substring in their filename. Default: None; files are not filtered.\n","    - extension_filter (``string``, optional): The desired file extension.\n","    Default: None; files are not filtered\n","    \"\"\"\n","    if not os.path.isdir(folder):\n","        raise RuntimeError(\"\\\"{0}\\\" is not a folder.\".format(folder))\n","\n","    # Filename filter: if not specified don't filter (condition always true);\n","    # otherwise, use a lambda expression to filter out files that do not\n","    # contain \"name_filter\"\n","    if name_filter is None:\n","        # This looks hackish...there is probably a better way\n","        name_cond = lambda filename: True\n","    else:\n","        name_cond = lambda filename: name_filter in filename\n","\n","    # Extension filter: if not specified don't filter (condition always true);\n","    # otherwise, use a lambda expression to filter out files whose extension\n","    # is not \"extension_filter\"\n","    if extension_filter is None:\n","        # This looks hackish...there is probably a better way\n","        ext_cond = lambda filename: True\n","    else:\n","        ext_cond = lambda filename: filename.endswith(extension_filter)\n","\n","    filtered_files = []\n","\n","    # Explore the directory tree to get files that contain \"name_filter\" and\n","    # with extension \"extension_filter\"\n","    for path, _, files in os.walk(folder):\n","        files.sort()\n","        for file in files:\n","            if name_cond(file) and ext_cond(file):\n","                full_path = os.path.join(path, file)\n","                filtered_files.append(full_path)\n","\n","    return filtered_files\n","\n","\n","def pil_loader(data_path, label_path, inst_path, dpth_path):\n","    \"\"\"Loads a sample and label image given their path as PIL images.\n","    Keyword arguments:\n","    - data_path (``string``): The filepath to the image.\n","    - label_path (``string``): The filepath to the ground-truth semantic image.\n","    - inst_path (``string``): The filepath to the ground-truth instance image.\n","    - dpth_path (``string``): The filepath to the ground-truth depth image.\n","    Returns the image and the label as PIL images.\n","    \"\"\"\n","    data = Image.open(data_path)\n","    label = Image.open(label_path)\n","    inst = Image.open(inst_path)\n","    dpth = Image.open(dpth_path)\n","\n","    return data, label, inst, dpth\n","\n","\n","def remap(image, old_values, new_values):\n","    assert isinstance(image, Image.Image) or isinstance(\n","        image, np.ndarray), \"image must be of type PIL.Image or numpy.ndarray\"\n","    assert type(new_values) is tuple, \"new_values must be of type tuple\"\n","    assert type(old_values) is tuple, \"old_values must be of type tuple\"\n","    assert len(new_values) == len(\n","        old_values), \"new_values and old_values must have the same length\"\n","\n","    # If image is a PIL.Image convert it to a numpy array\n","    if isinstance(image, Image.Image):\n","        image = np.array(image)\n","\n","    # Replace old values by the new ones\n","    tmp = np.zeros_like(image)\n","    for old, new in zip(old_values, new_values):\n","        # Since tmp is already initialized as zeros we can skip new values\n","        # equal to 0\n","        if new != 0:\n","            tmp[image == old] = new\n","\n","    return Image.fromarray(tmp)\n","\n","\n","def enet_weighing(dataloader, num_classes, c=1.02):\n","    \"\"\"Computes class weights as described in the ENet paper:\n","        w_class = 1 / (ln(c + p_class)),\n","    where c is usually 1.02 and p_class is the propensity score of that\n","    class:\n","        propensity_score = freq_class / total_pixels.\n","    References: https://arxiv.org/abs/1606.02147\n","    Keyword arguments:\n","    - dataloader (``data.Dataloader``): A data loader to iterate over the\n","    dataset.\n","    - num_classes (``int``): The number of classes.\n","    - c (``int``, optional): AN additional hyper-parameter which restricts\n","    the interval of values for the weights. Default: 1.02.\n","    \"\"\"\n","    class_count = 0\n","    total = 0\n","    for _, label in dataloader:\n","        label = label.cpu().numpy()\n","\n","        # Flatten label\n","        flat_label = label.flatten()\n","\n","        # Sum up the number of pixels of each class and the total pixel\n","        # counts for each label\n","        class_count += np.bincount(flat_label, minlength=num_classes)\n","        total += flat_label.size\n","\n","    # Compute propensity score and then the weights for each class\n","    propensity_score = class_count / total\n","    class_weights = 1 / (np.log(c + propensity_score))\n","\n","    return class_weights\n","\n","\n","def median_freq_balancing(dataloader, num_classes):\n","    \"\"\"Computes class weights using median frequency balancing as described\n","    in https://arxiv.org/abs/1411.4734:\n","        w_class = median_freq / freq_class,\n","    where freq_class is the number of pixels of a given class divided by\n","    the total number of pixels in images where that class is present, and\n","    median_freq is the median of freq_class.\n","    Keyword arguments:\n","    - dataloader (``data.Dataloader``): A data loader to iterate over the\n","    dataset.\n","    whose weights are going to be computed.\n","    - num_classes (``int``): The number of classes\n","    \"\"\"\n","    class_count = 0\n","    total = 0\n","    for _, label in dataloader:\n","        label = label.cpu().numpy()\n","\n","        # Flatten label\n","        flat_label = label.flatten()\n","\n","        # Sum up the class frequencies\n","        bincount = np.bincount(flat_label, minlength=num_classes)\n","\n","        # Create of mask of classes that exist in the label\n","        mask = bincount > 0\n","        # Multiply the mask by the pixel count. The resulting array has\n","        # one element for each class. The value is either 0 (if the class\n","        # does not exist in the label) or equal to the pixel count (if\n","        # the class exists in the label)\n","        total += mask * flat_label.size\n","\n","        # Sum up the number of pixels found for each class\n","        class_count += bincount\n","\n","    # Compute the frequency and its median\n","    freq = class_count / total\n","    med = np.median(freq)\n","\n","    return med / freq"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2s2_gazJEhXV","colab_type":"code","colab":{}},"source":["import os\n","from collections import OrderedDict\n","import torch.utils.data as data\n","#from data import utils\n","\n","\n","class Cityscapes(data.Dataset):\n","    \"\"\"Cityscapes dataset https://www.cityscapes-dataset.com/.\n","    Keyword arguments:\n","    - root_dir (``string``): Root directory path.\n","    - mode (``string``): The type of dataset: 'train' for training set, 'val'\n","    for validation set, and 'test' for test set.\n","    - transform (``callable``, optional): A function/transform that  takes in\n","    an PIL image and returns a transformed version. Default: None.\n","    - label_transform (``callable``, optional): A function/transform that takes\n","    in the target and transforms it. Default: None.\n","    - loader (``callable``, optional): A function to load an image given its\n","    path. By default ``default_loader`` is used.\n","    \"\"\"\n","    # Training dataset root folders\n","    train_folder = \"leftImg8bit_trainvaltest/leftImg8bit/train\"\n","    train_lbl_folder = \"gtFine_trainvaltest/gtFine/train\"\n","    train_dpth_folder = \"disparity_trainvaltest/disparity/train\"\n","\n","    # Validation dataset root folders\n","    val_folder = \"leftImg8bit_trainvaltest/leftImg8bit/val\"\n","    val_lbl_folder = \"gtFine_trainvaltest/gtFine/val\"\n","    val_dpth_folder = \"disparity_trainvaltest/disparity/val\"\n","\n","    # Test dataset root folders\n","    test_folder = \"leftImg8bit_trainvaltest/leftImg8bit/test\"\n","    test_lbl_folder = \"gtFine_trainvaltest/gtFine/test\"\n","    test_dpth_folder = \"disparity_trainvaltest/disparity/test\"\n","\n","    # Filters to find the images\n","    img_extension = '.png'\n","    lbl_name_filter = 'labelIds'\n","    inst_name_filter = 'instanceIds'\n","    dpth_name_filter = 'disparity'\n","\n","    # The values associated with the 35 classes\n","    full_classes = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n","                    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\n","                    32, 33, -1)\n","    # The values above are remapped to the following\n","    new_classes = (0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 4, 5, 0, 0, 0, 6, 0, 7,\n","                   8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 0, 17, 18, 19, 0)\n","\n","    # Default encoding for pixel value, class name, and class color\n","    color_encoding = OrderedDict([\n","            ('unlabeled', (0, 0, 0)),\n","            ('road', (128, 64, 128)),\n","            ('sidewalk', (244, 35, 232)),\n","            ('building', (70, 70, 70)),\n","            ('wall', (102, 102, 156)),\n","            ('fence', (190, 153, 153)),\n","            ('pole', (153, 153, 153)),\n","            ('traffic_light', (250, 170, 30)),\n","            ('traffic_sign', (220, 220, 0)),\n","            ('vegetation', (107, 142, 35)),\n","            ('terrain', (152, 251, 152)),\n","            ('sky', (70, 130, 180)),\n","            ('person', (220, 20, 60)),\n","            ('rider', (255, 0, 0)),\n","            ('car', (0, 0, 142)),\n","            ('truck', (0, 0, 70)),\n","            ('bus', (0, 60, 100)),\n","            ('train', (0, 80, 100)),\n","            ('motorcycle', (0, 0, 230)),\n","            ('bicycle', (119, 11, 32))\n","    ])\n","\n","    def __init__(self,\n","                 root_dir,\n","                 mode='train',\n","                 transform=None,\n","                 label_transform=None,\n","                 loader=pil_loader):\n","        self.root_dir = root_dir\n","        self.mode = mode\n","        self.transform = transform\n","        self.label_transform = label_transform\n","        self.loader = loader\n","\n","        if self.mode.lower() == 'train':\n","            # Get the training data and labels filepaths\n","            self.train_data = get_files(\n","                os.path.join(root_dir, self.train_folder),\n","                extension_filter=self.img_extension)\n","\n","            self.train_labels = get_files(\n","                os.path.join(root_dir, self.train_lbl_folder),\n","                name_filter=self.lbl_name_filter,\n","                extension_filter=self.img_extension)\n","\n","            self.train_instance = get_files(\n","                os.path.join(root_dir, self.train_lbl_folder),\n","                name_filter=self.inst_name_filter,\n","                extension_filter=self.img_extension)\n","\n","            self.train_depth = get_files(\n","                os.path.join(root_dir, self.train_dpth_folder),\n","                name_filter=self.dpth_name_filter,\n","                extension_filter=self.img_extension)\n","\n","\n","        elif self.mode.lower() == 'val':\n","            # Get the validation data and labels filepaths\n","            self.val_data = get_files(\n","                os.path.join(root_dir, self.val_folder),\n","                extension_filter=self.img_extension)\n","\n","            self.val_labels = get_files(\n","                os.path.join(root_dir, self.val_lbl_folder),\n","                name_filter=self.lbl_name_filter,\n","                extension_filter=self.img_extension)\n","\n","            self.val_instance = get_files(\n","                os.path.join(root_dir, self.val_lbl_folder),\n","                name_filter=self.inst_name_filter,\n","                extension_filter=self.img_extension)\n","\n","            self.val_depth = get_files(\n","                os.path.join(root_dir, self.val_dpth_folder),\n","                name_filter=self.dpth_name_filter,\n","                extension_filter=self.img_extension)\n","\n","\n","        elif self.mode.lower() == 'test':\n","            # Get the test data and labels filepaths\n","            self.test_data = get_files(\n","                os.path.join(root_dir, self.test_folder),\n","                extension_filter=self.img_extension)\n","\n","            self.test_labels = get_files(\n","                os.path.join(root_dir, self.test_lbl_folder),\n","                name_filter=self.lbl_name_filter,\n","                extension_filter=self.img_extension)\n","\n","            self.test_instance = get_files(\n","                os.path.join(root_dir, self.test_lbl_folder),\n","                name_filter=self.inst_name_filter,\n","                extension_filter=self.img_extension)\n","\n","            self.test_depth = get_files(\n","                os.path.join(root_dir, self.test_dpth_folder),\n","                name_filter=self.dpth_name_filter,\n","                extension_filter=self.img_extension)\n","\n","        else:\n","            raise RuntimeError(\"Unexpected dataset mode. \"\n","                               \"Supported modes are: train, val and test\")\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","        - index (``int``): index of the item in the dataset\n","        Returns:\n","        A tuple of ``PIL.Image`` (image, label) where label is the ground-truth\n","        of the image.\n","        \"\"\"\n","        if self.mode.lower() == 'train':\n","            data_path =  self.train_data[index]\n","            label_path = self.train_labels[index]\n","            inst_path =  self.train_instance[index]\n","            dpth_path  = self.train_depth[index]\n","\n","        elif self.mode.lower() == 'val':\n","            data_path =  self.val_data[index]\n","            label_path = self.val_labels[index] \n","            inst_path =  self.val_instance[index]\n","            dpth_path  = self.val_depth[index]\n","\n","        elif self.mode.lower() == 'test':\n","            data_path =  self.test_data[index]\n","            label_path = self.test_labels[index]\n","            inst_path =  self.test_instance[index]\n","            dpth_path  = self.test_depth[index]\n","\n","        else:\n","            raise RuntimeError(\"Unexpected dataset mode. \"\n","                               \"Supported modes are: train, val and test\")\n","\n","        print(data_path, label_path, inst_path, dpth_path)\n","\n","\n","\n","        img,label,inst,dpth = self.loader(data_path, label_path, inst_path, dpth_path)\n","\n","        # # Remap class labels\n","        # label = utils.remap(label, self.full_classes, self.new_classes)\n","\n","        img = self.transform(img)\n","        label = self.transform(label)\n","        inst = self.transform(inst)\n","        dpth = self.transform(dpth)\n","\n","\n","        # # if self.transform is not None:\n","        # #     img = self.transform(img)\n","\n","        # # if self.label_transform is not None:\n","        # #     label = self.label_transform(label)\n","\n","        return img, label, inst, dpth\n","\n","    def __len__(self):\n","        \"\"\"Returns the length of the dataset.\"\"\"\n","        if self.mode.lower() == 'train':\n","            return len(self.train_data)\n","        elif self.mode.lower() == 'val':\n","            return len(self.val_data)\n","        elif self.mode.lower() == 'test':\n","            return len(self.test_data)\n","        else:\n","            raise RuntimeError(\"Unexpected dataset mode. \"\n","                               \"Supported modes are: train, val and test\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDE1PY_d-ZKr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":475},"outputId":"1ee6e49a-ca8a-462b-d1a9-d74976bdef10","executionInfo":{"status":"error","timestamp":1586975228932,"user_tz":240,"elapsed":239,"user":{"displayName":"Shashwat Shivam","photoUrl":"","userId":"00567306531573054305"}}},"source":["height = 512\n","width = 1024\n","dataset_dir = 'data/cityscape'\n","image_transform = transforms.Compose(\n","        [transforms.Resize((height,width)),transforms.ToTensor()])\n","train_set = Cityscapes(dataset_dir,transform=image_transform)\n","\n","train_loader = DataLoader(train_set,batch_size=1,shuffle=True,\n","        num_workers=1)"],"execution_count":78,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-0d2b6621d345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m image_transform = transforms.Compose(\n\u001b[1;32m      5\u001b[0m         [transforms.Resize((height,width)),transforms.ToTensor()])\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCityscapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m train_loader = DataLoader(train_set,batch_size=1,shuffle=True,\n","\u001b[0;32m<ipython-input-77-923a9a028bd8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, mode, transform, label_transform, loader)\u001b[0m\n\u001b[1;32m     87\u001b[0m             self.train_data = get_files(\n\u001b[1;32m     88\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 extension_filter=self.img_extension)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             self.train_labels = get_files(\n","\u001b[0;32m<ipython-input-71-7d77b7fe9cb3>\u001b[0m in \u001b[0;36mget_files\u001b[0;34m(folder, name_filter, extension_filter)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\"{0}\\\" is not a folder.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Filename filter: if not specified don't filter (condition always true);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: \"data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train\" is not a folder."]}]},{"cell_type":"code","metadata":{"id":"5Gu0z1wy-bq-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DxnljE48jhlm","colab_type":"text"},"source":["## 5 - Define the loader that will load the input and output images(todo)"]},{"cell_type":"code","metadata":{"id":"0S-PxeTpjhln","colab_type":"code","colab":{}},"source":["def loader(training_path, segmented_path, batch_size, h=320, w=1000):\n","    filenames_t = os.listdir(training_path)\n","    total_files_t = len(filenames_t)\n","    \n","    filenames_s = os.listdir(segmented_path)\n","    total_files_s = len(filenames_s)\n","    \n","    assert(total_files_t == total_files_s)\n","    \n","    if str(batch_size).lower() == 'all':\n","        batch_size = total_files_s\n","    \n","    idx = 0\n","    while(1):\n","      # Choosing random indexes of images and labels\n","        batch_idxs = np.random.randint(0, total_files_s, batch_size)\n","            \n","        \n","        inputs = []\n","        labels = []\n","        \n","        for jj in batch_idxs:\n","          # Reading normalized photo\n","            img = plt.imread(training_path + filenames_t[jj])\n","          # Resizing using nearest neighbor method\n","            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n","            inputs.append(img)\n","          \n","          # Reading semantic image\n","            img = Image.open(segmented_path + filenames_s[jj])\n","            img = np.array(img)\n","          # Resizing using nearest neighbor method\n","            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n","            labels.append(img)\n","         \n","        inputs = np.stack(inputs, axis=2)\n","      # Changing image format to C x H x W\n","        inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n","        \n","        labels = torch.tensor(labels)\n","        \n","        yield inputs, labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jyRzVwcjhlq","colab_type":"text"},"source":["## 6 - Define the class weights(todo)"]},{"cell_type":"code","metadata":{"id":"uIrH8GkZjhlr","colab_type":"code","colab":{}},"source":["def get_class_weights(num_classes, c=1.02):\n","    pipe = loader('images/train/', 'images/trainannot/', batch_size='all')\n","    _, labels = next(pipe)\n","    all_labels = labels.flatten()\n","    print(all_labels)\n","    each_class = np.bincount(all_labels, minlength=num_classes)\n","    prospensity_score = each_class / len(all_labels)\n","    class_weights = 1 / (np.log(c + prospensity_score))\n","    return class_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsxpISPxjhlt","colab_type":"code","colab":{}},"source":["class_weights = get_class_weights(12)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hlATaBtjjhlw","colab_type":"text"},"source":["## 7 - Define the Hyperparameters(todo)"]},{"cell_type":"code","metadata":{"id":"lDZLtYRbjhlw","colab_type":"code","colab":{}},"source":["lr = 5e-4\n","batch_size = 1\n","\n","label_criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n","inst_criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n","dpth_criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n","\n","optimizer = torch.optim.Adam(enet.parameters(), \n","                             lr=lr,\n","                             weight_decay=2e-4)\n","\n","print_every = 5\n","eval_every = 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Odil0dEjhlz","colab_type":"text"},"source":["## 8 - Training loop(todo)"]},{"cell_type":"code","metadata":{"id":"PgTjjlmxjhlz","colab_type":"code","colab":{}},"source":["train_losses = []\n","eval_losses = []\n","\n","bc_train = 367 // batch_size # mini_batch train\n","bc_eval = 101 // batch_size  # mini_batch validation\n","\n","# Define pipeline objects\n","#pipe = loader('images/train/', 'images/trainannot/', batch_size)\n","#eval_pipe = loader('images/val/', 'images/valannot/', batch_size)\n","\n","dataiter = iter(train_loader)\n","img, label, inst, dpth = dataiter.next()\n","\n","#writer.add_graph(enet, img)\n","#writer.close()\n","\n","epochs = 100\n","\n","# Train loop\n","\n","for e in range(1, epochs+1):\n","    \n","    \n","    train_loss = 0\n","    print ('-'*15,'Epoch %d' % e, '-'*15)\n","    \n","    enet.train()\n","    \n","    for _ in tqdm(range(bc_train)):\n","        #X_batch, mask_batch = next(pipe)\n","        img, label, inst, dpth = dataiter.next()\n","\n","        # assign data to cpu/gpu\n","        #X_batch, mask_batch = X_batch.to(device), mask_batch.to(device)\n","        img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n","\n","        optimizer.zero_grad()\n","        \n","        out = enet(X_batch.float())\n","\n","        # loss calculation for depth\n","        loss = dpth_criterion(out[0], dpth.long())\n","        # update weights\n","        loss.backward(retain_graph=True)\n","        #optimizer.step()\n","\n","        train_loss += loss.item()\n","        \n","        # loss calculation for class instance\n","        loss = inst_criterion(out[1], inst.long())\n","        # update weights\n","        loss.backward(retain_graph=True)\n","        #optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","        # loss calculation for class segmentation\n","        loss = label_criterion(out[2], label.long())\n","        # update weights\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    train_losses.append(train_loss)\n","    \n","    if (e+1) % print_every == 0:\n","        print ('Epoch {}/{}...'.format(e, epochs),\n","                'Loss {:6f}'.format(train_loss))\n","    \n","    if e % eval_every == 0:\n","        with torch.no_grad():\n","            enet.eval()\n","            \n","            eval_loss = 0\n","\n","            # Validation loop\n","            for _ in tqdm(range(bc_eval)):\n","                inputs, labels = next(eval_pipe)\n","\n","                \n","                inputs, labels = inputs.to(device), labels.to(device)\n","                    \n","                \n","                out = enet(inputs)\n","                \n","                out = out.data.max(1)[1]\n","                \n","                eval_loss += (labels.long() - out.long()).sum()\n","                \n","            \n","            print ()\n","            print ('Loss {:6f}'.format(eval_loss))\n","            \n","            eval_losses.append(eval_loss)\n","        \n","    if e % print_every == 0:\n","        checkpoint = {\n","            'epochs' : e,\n","            'state_dict' : enet.state_dict()\n","        }\n","        torch.save(checkpoint, '/content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n","        print ('Model saved!')\n","\n","print ('Epoch {}/{}...'.format(e, epochs),\n","       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2UELZ5ghjhl2","colab_type":"code","colab":{}},"source":["print(F)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnJmyQw9b1jm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}