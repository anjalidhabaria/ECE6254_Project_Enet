{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wrqIinejhkt"
   },
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import future\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-dj3wA0jhk5"
   },
   "source": [
    "## Define the ENet model\n",
    "\n",
    "We decided to model following residual blocks as separate class to model ENET encoder and decoder:\n",
    "    - Initial block\n",
    "    - RDDNeck - class for regular, downsampling and dilated bottlenecks\n",
    "    - ASNeck - class for asymetric bottlenecks\n",
    "    - UBNeck - class for upsampling bottlenecks\n",
    "\n",
    "ENET architecture is autoencoder based model and is divided into 5 sub-blocks. Pleas refer [ENET paper](https://arxiv.org/pdf/1606.02147.pdf) for details of each sub-block. ENET building blocks code is taken from [here](https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation).\n",
    "\n",
    "Fast scene understanding uses first 2 sub-blocks as encoder and remaining 3 as decoder. In this implemantation, there is 1 shared encoder and 3 separate decoder for 3 tasks(instance segementation, semantic segmentation, Depth estimation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lin/ECE6254_Project_Enet\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "nb_dir = os.getcwd()\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "print(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ENetDecoder import ENetDecoder\n",
    "from models.ENetEncoder import ENetEncoder\n",
    "\n",
    "class BranchedENet(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        self.enc = ENetEncoder(C)\n",
    "        \n",
    "        self.dec1 = ENetDecoder(C)\n",
    "        self.dec2 = ENetDecoder(C)\n",
    "        self.dec3 = ENetDecoder(C)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Output of Encoder\n",
    "        x, i1, i2 = self.enc(x)\n",
    "        # output of all 3 decoder in list\n",
    "        #x = torch.stack([self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2)])\n",
    "        x = (self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnRAXNvbjhlf"
   },
   "source": [
    "## Instantiate the ENet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Zo6pIjqjhlg"
   },
   "outputs": [],
   "source": [
    "enet = BranchedENet(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQAvwXDpjhlj"
   },
   "outputs": [],
   "source": [
    "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enet = enet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bNQ-SAqjhky"
   },
   "source": [
    "## Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cityscapes import Cityscapes as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 1024\n",
    "dataset_dir = 'data/cityscape'\n",
    "image_transform = transforms.Compose(\n",
    "        [transforms.Resize((height,width)),transforms.ToTensor()])\n",
    "train_set = dataset(dataset_dir,transform=image_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set,batch_size=1,shuffle=True,\n",
    "        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bremen/bremen_000282_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000282_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000282_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bremen/bremen_000282_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/erfurt/erfurt_000022_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000022_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000022_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/erfurt/erfurt_000022_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bremen/bremen_000267_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000267_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000267_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bremen/bremen_000267_000019_disparity.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/.local/lib/python3.6/site-packages/torch/jit/__init__.py:1044: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 19, 3, 489] (0.019528158009052277 vs. 0.009444912895560265) and 91505 other locations (0.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n",
      "/home/lin/.local/lib/python3.6/site-packages/torch/jit/__init__.py:1044: TracerWarning: Output nr 2. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 27, 24, 504] (-0.0011107708560302854 vs. 0.0064564417116343975) and 77003 other locations (0.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n",
      "/home/lin/.local/lib/python3.6/site-packages/torch/jit/__init__.py:1044: TracerWarning: Output nr 3. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 18, 55, 505] (0.015072399750351906 vs. 0.007174754049628973) and 86947 other locations (0.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/erfurt/erfurt_000077_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000077_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000077_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/erfurt/erfurt_000077_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/strasbourg/strasbourg_000000_004112_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/strasbourg/strasbourg_000000_004112_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/strasbourg/strasbourg_000000_004112_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/strasbourg/strasbourg_000000_004112_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/jena/jena_000010_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/jena/jena_000010_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/jena/jena_000010_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/jena/jena_000010_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/aachen/aachen_000038_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/aachen/aachen_000038_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/aachen/aachen_000038_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/aachen/aachen_000038_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bochum/bochum_000000_033056_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bochum/bochum_000000_033056_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bochum/bochum_000000_033056_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bochum/bochum_000000_033056_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/aachen/aachen_000075_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/aachen/aachen_000075_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/aachen/aachen_000075_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/aachen/aachen_000075_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/hanover/hanover_000000_027650_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_027650_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_027650_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/hanover/hanover_000000_027650_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/hanover/hanover_000000_039021_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_039021_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_039021_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/hanover/hanover_000000_039021_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bremen/bremen_000247_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000247_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000247_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bremen/bremen_000247_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/erfurt/erfurt_000003_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000003_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000003_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/erfurt/erfurt_000003_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/cologne/cologne_000039_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/cologne/cologne_000039_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/cologne/cologne_000039_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/cologne/cologne_000039_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/hanover/hanover_000000_046398_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_046398_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_046398_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/hanover/hanover_000000_046398_disparity.png\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "img, label, inst, dpth = dataiter.next()\n",
    "\n",
    "writer.add_graph(enet, img.to(device))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to add in training loop\n",
    "# writer.add_scalar('training loss',running_loss / 1000,epoch * len(trainloader) + i)\n",
    "\n",
    "# for n_iter in range(100):\n",
    "#     writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpCP5JKAjhlW"
   },
   "source": [
    "## 3 - Losses(todo)\n",
    "(1) Semantic Segmentation Loss\n",
    "\n",
    "(2) Instantance Segmentation Loss\n",
    "\n",
    "(3) Depth Estimation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "0b6IvyXIjhlW",
    "outputId": "ace7c895-c043-4152-d5a5-e9339ebf6854"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the implementation of following paper:\n",
    "https://arxiv.org/pdf/1802.05591.pdf\n",
    "This implementation is based on following code:\n",
    "https://github.com/Wizaron/instance-segmentation-pytorch\n",
    "\"\"\"\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "\n",
    "class DiscriminativeLoss(_Loss):\n",
    "\n",
    "    def __init__(self, delta_var=0.5, delta_dist=1.5,\n",
    "                 norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n",
    "                 usegpu=True, size_average=True):\n",
    "        super(DiscriminativeLoss, self).__init__(size_average)\n",
    "        self.delta_var = delta_var\n",
    "        self.delta_dist = delta_dist\n",
    "        self.norm = norm\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.usegpu = usegpu\n",
    "        assert self.norm in [1, 2]\n",
    "\n",
    "    def forward(self, input, target, n_clusters):\n",
    "#         _assert_no_grad(target)\n",
    "        return self._discriminative_loss(input, target, n_clusters)\n",
    "\n",
    "    def _discriminative_loss(self, input, target, n_clusters):\n",
    "        bs, n_features, height, width = input.size()\n",
    "        max_n_clusters = target.size(1)\n",
    "\n",
    "        input = input.contiguous().view(bs, n_features, height * width)\n",
    "        target = target.contiguous().view(bs, max_n_clusters, height * width)\n",
    "\n",
    "        c_means = self._cluster_means(input, target, n_clusters)\n",
    "        l_var = self._variance_term(input, target, c_means, n_clusters)\n",
    "        l_dist = self._distance_term(c_means, n_clusters)\n",
    "        l_reg = self._regularization_term(c_means, n_clusters)\n",
    "\n",
    "        loss = self.alpha * l_var + self.beta * l_dist + self.gamma * l_reg\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _cluster_means(self, input, target, n_clusters):\n",
    "        bs, n_features, n_loc = input.size()\n",
    "        max_n_clusters = target.size(1)\n",
    "\n",
    "        # bs, n_features, max_n_clusters, n_loc\n",
    "        input = input.unsqueeze(2).expand(bs, n_features, max_n_clusters, n_loc)\n",
    "        # bs, 1, max_n_clusters, n_loc\n",
    "        target = target.unsqueeze(1)\n",
    "        # bs, n_features, max_n_clusters, n_loc\n",
    "        input = input * target\n",
    "\n",
    "        means = []\n",
    "        for i in range(bs):\n",
    "            # n_features, n_clusters, n_loc\n",
    "            input_sample = input[i, :, :n_clusters[i]]\n",
    "            # 1, n_clusters, n_loc,\n",
    "            target_sample = target[i, :, :n_clusters[i]]\n",
    "            # n_features, n_cluster\n",
    "            mean_sample = input_sample.sum(2) / (target_sample.sum(2) + 0.00001)\n",
    "\n",
    "            # padding\n",
    "            n_pad_clusters = max_n_clusters - n_clusters[i]\n",
    "            assert n_pad_clusters >= 0\n",
    "            if n_pad_clusters > 0:\n",
    "                pad_sample = torch.zeros(n_features, n_pad_clusters)\n",
    "                pad_sample = Variable(pad_sample)\n",
    "                if self.usegpu:\n",
    "                    pad_sample = pad_sample.cuda()\n",
    "                mean_sample = torch.cat((mean_sample, pad_sample), dim=1)\n",
    "            means.append(mean_sample)\n",
    "\n",
    "        # bs, n_features, max_n_clusters\n",
    "        means = torch.stack(means)\n",
    "\n",
    "        return means\n",
    "\n",
    "    def _variance_term(self, input, target, c_means, n_clusters):\n",
    "        bs, n_features, n_loc = input.size()\n",
    "        max_n_clusters = target.size(1)\n",
    "\n",
    "        # bs, n_features, max_n_clusters, n_loc\n",
    "        c_means = c_means.unsqueeze(3).expand(bs, n_features, max_n_clusters, n_loc)\n",
    "        # bs, n_features, max_n_clusters, n_loc\n",
    "        input = input.unsqueeze(2).expand(bs, n_features, max_n_clusters, n_loc)\n",
    "        # bs, max_n_clusters, n_loc\n",
    "        var = (torch.clamp(torch.norm((input - c_means), self.norm, 1) -\n",
    "                           self.delta_var, min=0) ** 2) * target\n",
    "\n",
    "        var_term = 0\n",
    "        for i in range(bs):\n",
    "            # n_clusters, n_loc\n",
    "            var_sample = var[i, :n_clusters[i]]\n",
    "            # n_clusters, n_loc\n",
    "            target_sample = target[i, :n_clusters[i]]\n",
    "\n",
    "            # n_clusters\n",
    "            c_var = var_sample.sum(1) / (target_sample.sum(1) + 0.00001)\n",
    "            var_term += c_var.sum() / int(n_clusters[i])\n",
    "        var_term /= bs\n",
    "\n",
    "        return var_term\n",
    "\n",
    "    def _distance_term(self, c_means, n_clusters):\n",
    "        bs, n_features, max_n_clusters = c_means.size()\n",
    "\n",
    "        dist_term = 0\n",
    "        for i in range(bs):\n",
    "            if n_clusters[i] <= 1:\n",
    "                continue\n",
    "\n",
    "            # n_features, n_clusters\n",
    "            mean_sample = c_means[i, :, :n_clusters[i]]\n",
    "\n",
    "            # n_features, n_clusters, n_clusters\n",
    "            means_a = mean_sample.unsqueeze(2).expand(n_features, n_clusters[i], n_clusters[i])\n",
    "            means_b = means_a.permute(0, 2, 1)\n",
    "            diff = means_a - means_b\n",
    "\n",
    "            margin = 2 * self.delta_dist * (1.0 - torch.eye(n_clusters[i]))\n",
    "            margin = Variable(margin)\n",
    "            if self.usegpu:\n",
    "                margin = margin.cuda()\n",
    "            c_dist = torch.sum(torch.clamp(margin - torch.norm(diff, self.norm, 0), min=0) ** 2)\n",
    "            dist_term += c_dist / (2 * n_clusters[i] * (n_clusters[i] - 1))\n",
    "        dist_term /= bs\n",
    "\n",
    "        return dist_term\n",
    "\n",
    "    def _regularization_term(self, c_means, n_clusters):\n",
    "        bs, n_features, max_n_clusters = c_means.size()\n",
    "\n",
    "        reg_term = 0\n",
    "        for i in range(bs):\n",
    "            # n_features, n_clusters\n",
    "            mean_sample = c_means[i, :, :n_clusters[i]]\n",
    "            reg_term += torch.mean(torch.norm(mean_sample, self.norm, 0))\n",
    "        reg_term /= bs\n",
    "\n",
    "        return reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "4Dv3_I_Ljhla",
    "outputId": "a4a00785-0446-46aa-edd0-6900a1d1a78d"
   },
   "outputs": [],
   "source": [
    "def inverse_huber_loss(out, target):\n",
    "    absdiff = torch.abs(out-target)\n",
    "    C = 0.2*torch.max(absdiff)\n",
    "    return torch.mean(torch.where(absdiff<C, absdiff, (absdiff*absdiff+C*C)/(2*C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "lHm_GLNBjhld",
    "outputId": "3962711f-0616-4f3b-9570-426ec4902f9e"
   },
   "outputs": [],
   "source": [
    "# local grad = require 'autograd'\n",
    "\n",
    "# def lossfunction(lossf_name, weights):\n",
    "#     if (lossf_name == 'softmaxLoss') then\n",
    "#         lossfunction = cudnn.SpatialCrossEntropyCriterion(weights)\n",
    "#     elseif (lossf_name == 'huberLoss') then\n",
    "#         lossfunction = grad.nn.AutoCriterion('depthLoss_huber')(require 'lossf/myHuberLoss')\n",
    "#     elseif (lossf_name == 'instanceLoss') then\n",
    "#         lossfunction = grad.nn.AutoCriterion('instance_loss')(require 'lossf/myInstanceLoss')\n",
    "#     else\n",
    "#         assert(false, 'Cannot load lossfunction ' .. opts.lossf)\n",
    "#     end\n",
    "\n",
    "#     return lossfunction\n",
    "# end\n",
    "\n",
    "# return getLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxnljE48jhlm"
   },
   "source": [
    "# Step 5 and 6 has been done in dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlATaBtjjhlw"
   },
   "source": [
    "## 7 - Define the Hyperparameters(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDZLtYRbjhlw"
   },
   "outputs": [],
   "source": [
    "from data.utils import enet_weighing\n",
    "lr = 5e-4\n",
    "batch_size = 1\n",
    "\n",
    "# figure out enet_weighing issue\n",
    "#criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(enet_weighing(train_loader, 12)).to(device))\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion_dpth = torch.nn.MSELoss(reduction='mean').to(device)\n",
    "'''\n",
    "criterion_disc = DiscriminativeLoss(delta_var=0.1,\n",
    "                                       delta_dist=0.6,\n",
    "                                       norm=2,\n",
    "                                       usegpu=True).to(device)\n",
    "'''\n",
    "optimizer = torch.optim.Adam(enet.parameters(), \n",
    "                             lr=lr,\n",
    "                             weight_decay=2e-4)\n",
    "\n",
    "print_every = 5\n",
    "eval_every = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Odil0dEjhlz"
   },
   "source": [
    "## 8 - Training loop(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418,
     "referenced_widgets": [
      "ef1e273f3b2043b192bf81d8f4e124c5",
      "c6d5811378a4442aa4b699e457cadb69",
      "ca676c3ef7ca4d82b99142c3a287ecb7",
      "3b32064c3e2b45f9b67b2eb7c2b2bc93",
      "94113c6f17df4fa5999df7d94523af1e",
      "f41f55ceab4940c8a1533c347a19a4f2",
      "cdc46da27c4444e0b0e6f0fd8388a1b1",
      "88552b181b5f41aa94a1ca52e3bca36d"
     ]
    },
    "colab_type": "code",
    "id": "PgTjjlmxjhlz",
    "outputId": "d1272f7a-1d7e-4ceb-f8ca-68e6748cf0f3"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "bc_train = 367 // batch_size # mini_batch train\n",
    "bc_eval = 101 // batch_size  # mini_batch validation\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "2UELZ5ghjhl2",
    "outputId": "92120683-83af-44b1-ce71-ad275a5414c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e1b500182641008ff5ab1f72463d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=367.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8231cef50e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# split output into three predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5f3c4e67e783>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# output of all 3 decoder in list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#x = torch.stack([self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ECE6254_Project_Enet/models/ENetDecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, i1, i2)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# The fifth bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb51\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ECE6254_Project_Enet/models/UBNeck.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, indices)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvt3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    print ('-'*15,'Epoch %d' % e, '-'*15)\n",
    "    \n",
    "    enet.train()\n",
    "    \n",
    "    for _ in tqdm(range(bc_train)):\n",
    "        img, label, inst, dpth = dataiter.next()\n",
    "\n",
    "        # assign data to cpu/gpu\n",
    "        img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n",
    "        label = label.squeeze(1)\n",
    "        inst = inst.squeeze(1)\n",
    "        dpth = dpth.squeeze(1)\n",
    "        \n",
    "        # set non-car labels to 0 for inst\n",
    "        inst[inst!=26] = 0\n",
    "        #inst[inst!=15] = 0\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = enet(img.float())\n",
    "\n",
    "        # split output into three predictions\n",
    "        label_out, inst_out, dpth_out = out[0], out[1], out[2]\n",
    "    \n",
    "        # get pixel-wise sum for depth\n",
    "        dpth_out = torch.sum(dpth_out, dim=1)\n",
    "\n",
    "        # loss calculation for class segmentation\n",
    "        loss = criterion(label_out, label.long()).float()\n",
    "\n",
    "        # loss calculation for class instance\n",
    "        loss += criterion(inst_out, inst.long()).float()\n",
    "        \n",
    "        #[5] in the below line indicates no. of clusters or no. of instances. Should be taken from the image\n",
    "        #loss += criterion_disc(inst_out, inst.long(), [5] * len(inst)).float()\n",
    "\n",
    "        # loss calculation for depth\n",
    "        loss += criterion_dpth(dpth_out, dpth.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    writer.add_scalar('Loss/train', train_loss, e)\n",
    "    \n",
    "    if e % eval_every == 0:\n",
    "        with torch.no_grad():\n",
    "            enet.eval()\n",
    "            \n",
    "            eval_loss = 0\n",
    "\n",
    "            # Validation loop\n",
    "            for _ in tqdm(range(bc_eval)):\n",
    "                img, label, inst, dpth = dataiter.next()\n",
    "                img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n",
    "                label = label.squeeze(1)\n",
    "                inst = inst.squeeze(1)\n",
    "                dpth = dpth.squeeze(1)\n",
    "        \n",
    "                out = enet(img.float())\n",
    "                \n",
    "                # split output into three predictions\n",
    "                label_out, inst_out, dpth_out = out[0,:,:,:,:], out[1,:,:,:,:], out[2,:,:,:,:]\n",
    "\n",
    "                # get pixel-wise sum for depth\n",
    "                dpth_out = torch.sum(dpth_out, dim=1)\n",
    "\n",
    "                # loss calculation for class segmentation\n",
    "                eval_loss += criterion(label_out, label.long()).float().item()\n",
    "\n",
    "                # loss calculation for class instance\n",
    "                eval_loss += criterion(inst_out, inst.long()).float().item()\n",
    "                #eval_loss += criterion_disc(inst_out, inst.long(), [5] * len(inst)).float().item()\n",
    "\n",
    "                # loss calculation for depth\n",
    "                eval_loss += criterion_dpth(dpth_out, dpth.float()).item()\n",
    "                \n",
    "            \n",
    "            writer.add_scalar('Loss/test', eval_loss, e // eval_every)\n",
    "        \n",
    "    if e % print_every == 0:\n",
    "        checkpoint = {\n",
    "            'epochs' : e,\n",
    "            'state_dict' : enet.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, '/content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
    "        print ('Model saved!')\n",
    "\n",
    "print ('Epoch {}/{}...'.format(e, epochs),\n",
    "       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, _, files in os.walk(folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def discriminative_loss_single(prediction, correct_label, feature_dim, label_shape, \n",
    "                            delta_v, delta_d, param_var, param_dist, param_reg):\n",
    "    \n",
    "    ''' Discriminative loss for a single prediction/label pair.\n",
    "    :param prediction: inference of network\n",
    "    :param correct_label: instance label\n",
    "    :feature_dim: feature dimension of prediction\n",
    "    :param label_shape: shape of label\n",
    "    :param delta_v: cutoff variance distance\n",
    "    :param delta_d: curoff cluster distance\n",
    "    :param param_var: weight for intra cluster variance\n",
    "    :param param_dist: weight for inter cluster distances\n",
    "    :param param_reg: weight regularization\n",
    "    '''\n",
    "\n",
    "    ### Reshape so pixels are aligned along a vector\n",
    "    correct_label = correct_label.reshape([label_shape[1]*label_shape[0]])\n",
    "    print('correct_label')\n",
    "    print(correct_label)\n",
    "    print('\\n')\n",
    "    reshaped_pred = prediction.reshape([label_shape[1]*label_shape[0], feature_dim])\n",
    "    print('reshaped_pred')\n",
    "    print(reshaped_pred)\n",
    "    print('\\n')\n",
    "    ### Count instances\n",
    "    unique_labels, unique_id, counts = correct_label.unique(sorted=False, return_inverse=True, return_counts=True)\n",
    "    #counts = torch.stack([(correct_label==x_u).sum() for x_u in unique_labels])\n",
    "    print('unique_labels')\n",
    "    print(unique_labels)\n",
    "    print('\\n')\n",
    "    print('unique_id')\n",
    "    print(unique_id)\n",
    "    print('\\n')\n",
    "    print('counts')\n",
    "    print(counts)\n",
    "    print('\\n')\n",
    "    counts = counts.float()\n",
    "    print('counts')\n",
    "    print(counts)\n",
    "    print('\\n')\n",
    "    num_instances = torch.tensor(torch.numel(unique_labels))\n",
    "    print('num_instances')\n",
    "    print(num_instances)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    segmented_sum = torch.zeros_like(reshaped_pred).scatter_add_(1, unique_id, reshaped_pred)\n",
    "    \n",
    "    \n",
    "    mu = torch.floor(torch.div(segmented_sum, torch.reshape(counts, (-1, 1))))\n",
    "    mu_expand = torch.gather(mu, unique_id)\n",
    "\n",
    "    ### Calculate l_var\n",
    "    distance = torch.norm(tf.subtract(mu_expand, reshaped_pred), dim=1)\n",
    "    distance = torch.sub(distance, delta_v)\n",
    "    distance = torch.clamp(distance, min=0., max=distance)\n",
    "    distance = torch.mul(distance, distance)\n",
    "\n",
    "    l_var = tf.unsorted_segment_sum(distance, unique_id, num_instances)\n",
    "    l_var = torch.floor(torch.div(l_var, counts))\n",
    "    l_var = l_var.sum()\n",
    "    l_var = torch.divide(l_var, num_instances.float())\n",
    "    \n",
    "    ### Calculate l_dist\n",
    "    \n",
    "    # Get distance for each pair of clusters like this:\n",
    "    #   mu_1 - mu_1\n",
    "    #   mu_2 - mu_1\n",
    "    #   mu_3 - mu_1\n",
    "    #   mu_1 - mu_2\n",
    "    #   mu_2 - mu_2\n",
    "    #   mu_3 - mu_2\n",
    "    #   mu_1 - mu_3\n",
    "    #   mu_2 - mu_3\n",
    "    #   mu_3 - mu_3\n",
    "\n",
    "    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])\n",
    "    mu_band_rep = tf.tile(mu, [1, num_instances])\n",
    "    mu_band_rep = mu_band_rep.reshape([num_instances*num_instances, feature_dim])\n",
    "\n",
    "    mu_diff = torch.sub(mu_band_rep, mu_interleaved_rep)\n",
    "    \n",
    "    # Filter out zeros from same cluster subtraction\n",
    "    intermediate_tensor = torch.sum(tf.abs(mu_diff),axis=1)\n",
    "    zero_vector = torch.zeros(1, dtype=tf.float32)\n",
    "    bool_mask = torch.ne(intermediate_tensor, zero_vector)\n",
    "    mu_diff_bool = tf.torch.masked_select(mu_diff, bool_mask)\n",
    "\n",
    "    mu_norm = torch.norm(mu_diff_bool, p=2, axis=1)\n",
    "    mu_norm = torch.sub(2.*delta_d, mu_norm)\n",
    "    mu_norm = torch.clamp(mu_norm, min=0., max=mu_norm)\n",
    "    mu_norm = torch.mul(mu_norm,mu_norm)\n",
    "\n",
    "    l_dist = torch.mean(mu_norm)\n",
    "\n",
    "    ### Calculate l_reg\n",
    "    l_reg = torch.mean(torch.norm(mu, p=2, dim=1))\n",
    "\n",
    "    param_scale = 1.\n",
    "    l_var = param_var * l_var\n",
    "    l_dist = param_dist * l_dist\n",
    "    l_reg = param_reg * l_reg\n",
    "\n",
    "    loss = param_scale*(l_var + l_dist + l_reg)\n",
    "      \n",
    "    return True\n",
    "    \n",
    "\n",
    "\n",
    "def discriminative_loss(prediction, correct_label, feature_dim, image_shape, \n",
    "                delta_v, delta_d, param_var, param_dist, param_reg):\n",
    "    ''' Iterate over a batch of prediction/label and cumulate loss\n",
    "    :return: discriminative loss and its three components\n",
    "    '''\n",
    "    def cond(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
    "        return tf.less(i, tf.shape(batch)[0])\n",
    "\n",
    "    def body(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
    "        disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(prediction[i], correct_label[i], feature_dim, image_shape, \n",
    "                        delta_v, delta_d, param_var, param_dist, param_reg)\n",
    "\n",
    "        out_loss = out_loss.write(i, disc_loss)\n",
    "        out_var = out_var.write(i, l_var)\n",
    "        out_dist = out_dist.write(i, l_dist)\n",
    "        out_reg = out_reg.write(i, l_reg)\n",
    "\n",
    "        return label, batch, out_loss, out_var, out_dist, out_reg, i + 1\n",
    "\n",
    "    # TensorArray is a data structure that support dynamic writing\n",
    "    output_ta_loss = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_var = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_dist = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_reg = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "\n",
    "    _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _  = tf.while_loop(cond, body, [correct_label, \n",
    "                                                        prediction, \n",
    "                                                        output_ta_loss, \n",
    "                                                        output_ta_var, \n",
    "                                                        output_ta_dist, \n",
    "                                                        output_ta_reg, \n",
    "                                                        0])\n",
    "    out_loss_op = out_loss_op.stack()\n",
    "    out_var_op = out_var_op.stack()\n",
    "    out_dist_op = out_dist_op.stack()\n",
    "    out_reg_op = out_reg_op.stack()\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(out_loss_op)\n",
    "    l_var = tf.reduce_mean(out_var_op)\n",
    "    l_dist = tf.reduce_mean(out_dist_op)\n",
    "    l_reg = tf.reduce_mean(out_reg_op)\n",
    "\n",
    "    return disc_loss, l_var, l_dist, l_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_label\n",
      "tensor([   1, 2061,    2, 2062])\n",
      "\n",
      "\n",
      "reshaped_pred\n",
      "tensor([[   1],\n",
      "        [2061],\n",
      "        [   2],\n",
      "        [2062]])\n",
      "\n",
      "\n",
      "unique_labels\n",
      "tensor([   1, 2061,    2, 2062])\n",
      "\n",
      "\n",
      "unique_id\n",
      "tensor([0, 1, 2, 3])\n",
      "\n",
      "\n",
      "counts\n",
      "tensor([1, 1, 1, 1])\n",
      "\n",
      "\n",
      "counts\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "\n",
      "num_instances\n",
      "tensor(4)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: Index tensor must have same dimensions as output tensor at C:\\w\\1\\s\\tmp_conda_3.7_055457\\conda\\conda-bld\\pytorch_1565416617654\\work\\aten\\src\\TH/generic/THTensorEvenMoreMath.cpp:515",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-0bb2e7ff70cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcorrect_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2061\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2062\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(prediction, correct_label, 1, (2,2), \n\u001b[1;32m----> 4\u001b[1;33m                             1, 1, 0.001, 0.5, 1.5)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-2ff44983071c>\u001b[0m in \u001b[0;36mdiscriminative_loss_single\u001b[1;34m(prediction, correct_label, feature_dim, label_shape, delta_v, delta_d, param_var, param_dist, param_reg)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0msegmented_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreshaped_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshaped_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     '''\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 3: Index tensor must have same dimensions as output tensor at C:\\w\\1\\s\\tmp_conda_3.7_055457\\conda\\conda-bld\\pytorch_1565416617654\\work\\aten\\src\\TH/generic/THTensorEvenMoreMath.cpp:515"
     ]
    }
   ],
   "source": [
    "prediction = torch.tensor([[1,2061],[2,2062]])\n",
    "correct_label = torch.tensor([[1,2061],[2,2062]])\n",
    "disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(prediction, correct_label, 1, (2,2), \n",
    "                            1, 1, 0.001, 0.5, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
