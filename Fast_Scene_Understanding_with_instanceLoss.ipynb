{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wrqIinejhkt"
   },
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import future\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-dj3wA0jhk5"
   },
   "source": [
    "## Define the ENet model\n",
    "\n",
    "We decided to model following residual blocks as separate class to model ENET encoder and decoder:\n",
    "    - Initial block\n",
    "    - RDDNeck - class for regular, downsampling and dilated bottlenecks\n",
    "    - ASNeck - class for asymetric bottlenecks\n",
    "    - UBNeck - class for upsampling bottlenecks\n",
    "\n",
    "ENET architecture is autoencoder based model and is divided into 5 sub-blocks. Pleas refer [ENET paper](https://arxiv.org/pdf/1606.02147.pdf) for details of each sub-block. ENET building blocks code is taken from [here](https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation).\n",
    "\n",
    "Fast scene understanding uses first 2 sub-blocks as encoder and remaining 3 as decoder. In this implemantation, there is 1 shared encoder and 3 separate decoder for 3 tasks(instance segementation, semantic segmentation, Depth estimation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "nb_dir = os.getcwd()\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "print(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ENetDecoder import ENetDecoder\n",
    "from models.ENetEncoder import ENetEncoder\n",
    "\n",
    "class BranchedENet(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        self.enc = ENetEncoder(C)\n",
    "        self.dec1 = ENetDecoder(C)\n",
    "        self.dec2 = ENetDecoder(C)\n",
    "        self.dec3 = ENetDecoder(C)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Output of Encoder\n",
    "        x, i1, i2 = self.enc(x)\n",
    "        # output of all 3 decoder in list\n",
    "        #x = torch.stack([self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2)])\n",
    "        x = (self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnRAXNvbjhlf"
   },
   "source": [
    "## Instantiate the ENet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Zo6pIjqjhlg"
   },
   "outputs": [],
   "source": [
    "enet = BranchedENet(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQAvwXDpjhlj"
   },
   "outputs": [],
   "source": [
    "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enet = enet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bNQ-SAqjhky"
   },
   "source": [
    "## Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cityscapes import Cityscapes as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 1024\n",
    "dataset_dir = 'data/cityscape'\n",
    "image_transform = transforms.Compose(\n",
    "        [transforms.Resize((height,width)),transforms.ToTensor()])\n",
    "train_set = dataset(dataset_dir,transform=image_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set,batch_size=1,shuffle=True,\n",
    "        num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpCP5JKAjhlW"
   },
   "source": [
    "## 3 - Losses(todo)\n",
    "(1) Semantic Segmentation Loss\n",
    "\n",
    "(2) Instantance Segmentation Loss\n",
    "\n",
    "(3) Depth Estimation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "0b6IvyXIjhlW",
    "outputId": "ace7c895-c043-4152-d5a5-e9339ebf6854"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the implementation of following paper:\n",
    "https://arxiv.org/pdf/1802.05591.pdf\n",
    "This implementation is based on following code:\n",
    "https://github.com/Wizaron/instance-segmentation-pytorch\n",
    "\"\"\"\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "\n",
    "class DiscriminativeLoss(_Loss):\n",
    "\n",
    "    def __init__(self, delta_var=0.5, delta_dist=1.5,\n",
    "                 norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n",
    "                 usegpu=True, size_average=True):\n",
    "        super(DiscriminativeLoss, self).__init__(size_average)\n",
    "        self.delta_var = delta_var\n",
    "        self.delta_dist = delta_dist\n",
    "        self.norm = norm\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.usegpu = usegpu\n",
    "        assert self.norm in [1, 2]\n",
    "\n",
    "    def forward(self, input, target, n_clusters):\n",
    "#         _assert_no_grad(target)\n",
    "        return self._discriminative_loss(input, target, n_clusters)\n",
    "\n",
    "    def _discriminative_loss(self, input, target, n_clusters):\n",
    "        bs, n_features, height, width = input.size()\n",
    "        max_n_clusters = target.size(1)\n",
    "\n",
    "        input = input.contiguous().view(bs, n_features, height * width)\n",
    "        target = target.contiguous().view(bs, max_n_clusters, height * width)\n",
    "\n",
    "        c_means = self._cluster_means(input, target, n_clusters)\n",
    "        l_var = self._variance_term(input, target, c_means, n_clusters)\n",
    "        l_dist = self._distance_term(c_means, n_clusters)\n",
    "        l_reg = self._regularization_term(c_means, n_clusters)\n",
    "\n",
    "        loss = self.alpha * l_var + self.beta * l_dist + self.gamma * l_reg\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _cluster_means(self, input, target, n_clusters):\n",
    "        bs, n_features, n_loc = input.size()\n",
    "        max_n_clusters = target.size(1)\n",
    "\n",
    "        # bs, n_features, max_n_clusters, n_loc\n",
    "        input = input.unsqueeze(2).expand(bs, n_features, max_n_clusters, n_loc)\n",
    "        # bs, 1, max_n_clusters, n_loc\n",
    "        target = target.unsqueeze(1)\n",
    "        # bs, n_features, max_n_clusters, n_loc\n",
    "        input = input * target\n",
    "\n",
    "        means = []\n",
    "        for i in range(bs):\n",
    "            # n_features, n_clusters, n_loc\n",
    "            input_sample = input[i, :, :n_clusters[i]]\n",
    "            # 1, n_clusters, n_loc,\n",
    "            target_sample = target[i, :, :n_clusters[i]]\n",
    "            # n_features, n_cluster\n",
    "            mean_sample = input_sample.sum(2) / (target_sample.sum(2) + 0.00001)\n",
    "\n",
    "            # padding\n",
    "            n_pad_clusters = max_n_clusters - n_clusters[i]\n",
    "            assert n_pad_clusters >= 0\n",
    "            if n_pad_clusters > 0:\n",
    "                pad_sample = torch.zeros(n_features, n_pad_clusters)\n",
    "                pad_sample = Variable(pad_sample)\n",
    "                if self.usegpu:\n",
    "                    pad_sample = pad_sample.cuda()\n",
    "                mean_sample = torch.cat((mean_sample, pad_sample), dim=1)\n",
    "            means.append(mean_sample)\n",
    "\n",
    "        # bs, n_features, max_n_clusters\n",
    "        means = torch.stack(means)\n",
    "\n",
    "        return means\n",
    "\n",
    "    def _variance_term(self, input, target, c_means, n_clusters):\n",
    "        bs, n_features, n_loc = input.size()\n",
    "        max_n_clusters = target.size(1)\n",
    "\n",
    "        # bs, n_features, max_n_clusters, n_loc\n",
    "        c_means = c_means.unsqueeze(3).expand(bs, n_features, max_n_clusters, n_loc)\n",
    "        # bs, n_features, max_n_clusters, n_loc\n",
    "        input = input.unsqueeze(2).expand(bs, n_features, max_n_clusters, n_loc)\n",
    "        # bs, max_n_clusters, n_loc\n",
    "        var = (torch.clamp(torch.norm((input - c_means), self.norm, 1) -\n",
    "                           self.delta_var, min=0) ** 2) * target\n",
    "\n",
    "        var_term = 0\n",
    "        for i in range(bs):\n",
    "            # n_clusters, n_loc\n",
    "            var_sample = var[i, :n_clusters[i]]\n",
    "            # n_clusters, n_loc\n",
    "            target_sample = target[i, :n_clusters[i]]\n",
    "\n",
    "            # n_clusters\n",
    "            c_var = var_sample.sum(1) / (target_sample.sum(1) + 0.00001)\n",
    "            var_term += c_var.sum() / int(n_clusters[i])\n",
    "        var_term /= bs\n",
    "\n",
    "        return var_term\n",
    "\n",
    "    def _distance_term(self, c_means, n_clusters):\n",
    "        bs, n_features, max_n_clusters = c_means.size()\n",
    "\n",
    "        dist_term = 0\n",
    "        for i in range(bs):\n",
    "            if n_clusters[i] <= 1:\n",
    "                continue\n",
    "\n",
    "            # n_features, n_clusters\n",
    "            mean_sample = c_means[i, :, :n_clusters[i]]\n",
    "\n",
    "            # n_features, n_clusters, n_clusters\n",
    "            means_a = mean_sample.unsqueeze(2).expand(n_features, n_clusters[i], n_clusters[i])\n",
    "            means_b = means_a.permute(0, 2, 1)\n",
    "            diff = means_a - means_b\n",
    "\n",
    "            margin = 2 * self.delta_dist * (1.0 - torch.eye(n_clusters[i]))\n",
    "            margin = Variable(margin)\n",
    "            if self.usegpu:\n",
    "                margin = margin.cuda()\n",
    "            c_dist = torch.sum(torch.clamp(margin - torch.norm(diff, self.norm, 0), min=0) ** 2)\n",
    "            dist_term += c_dist / (2 * n_clusters[i] * (n_clusters[i] - 1))\n",
    "        dist_term /= bs\n",
    "\n",
    "        return dist_term\n",
    "\n",
    "    def _regularization_term(self, c_means, n_clusters):\n",
    "        bs, n_features, max_n_clusters = c_means.size()\n",
    "\n",
    "        reg_term = 0\n",
    "        for i in range(bs):\n",
    "            # n_features, n_clusters\n",
    "            mean_sample = c_means[i, :, :n_clusters[i]]\n",
    "            reg_term += torch.mean(torch.norm(mean_sample, self.norm, 0))\n",
    "        reg_term /= bs\n",
    "\n",
    "        return reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "4Dv3_I_Ljhla",
    "outputId": "a4a00785-0446-46aa-edd0-6900a1d1a78d"
   },
   "outputs": [],
   "source": [
    "def inverse_huber_loss(out, target):\n",
    "    absdiff = torch.abs(out-target)\n",
    "    C = 0.2*torch.max(absdiff)\n",
    "    return torch.mean(torch.where(absdiff<C, absdiff, (absdiff*absdiff+C*C)/(2*C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "lHm_GLNBjhld",
    "outputId": "3962711f-0616-4f3b-9570-426ec4902f9e"
   },
   "outputs": [],
   "source": [
    "# local grad = require 'autograd'\n",
    "\n",
    "# def lossfunction(lossf_name, weights):\n",
    "#     if (lossf_name == 'softmaxLoss') then\n",
    "#         lossfunction = cudnn.SpatialCrossEntropyCriterion(weights)\n",
    "#     elseif (lossf_name == 'huberLoss') then\n",
    "#         lossfunction = grad.nn.AutoCriterion('depthLoss_huber')(require 'lossf/myHuberLoss')\n",
    "#     elseif (lossf_name == 'instanceLoss') then\n",
    "#         lossfunction = grad.nn.AutoCriterion('instance_loss')(require 'lossf/myInstanceLoss')\n",
    "#     else\n",
    "#         assert(false, 'Cannot load lossfunction ' .. opts.lossf)\n",
    "#     end\n",
    "\n",
    "#     return lossfunction\n",
    "# end\n",
    "\n",
    "# return getLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxnljE48jhlm"
   },
   "source": [
    "# Step 5 and 6 has been done in dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlATaBtjjhlw"
   },
   "source": [
    "## 7 - Define the Hyperparameters(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDZLtYRbjhlw"
   },
   "outputs": [],
   "source": [
    "from data.utils import enet_weighing\n",
    "lr = 5e-4\n",
    "batch_size = 1\n",
    "\n",
    "# figure out enet_weighing issue\n",
    "#criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(enet_weighing(train_loader, 12)).to(device))\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion_dpth = torch.nn.MSELoss(reduction='mean').to(device)\n",
    "'''\n",
    "criterion_disc = DiscriminativeLoss(delta_var=0.1,\n",
    "                                       delta_dist=0.6,\n",
    "                                       norm=2,\n",
    "                                       usegpu=True).to(device)\n",
    "'''\n",
    "optimizer = torch.optim.Adam(enet.parameters(), lr=lr, weight_decay=2e-4)\n",
    "\n",
    "## Name experiment to differentiate different runs for tensorboard\n",
    "## eg. for hyperparameter tuning\n",
    "experiment = 'experiment_lr-' + str(lr)\n",
    "\n",
    "print_every = 5\n",
    "eval_every = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Odil0dEjhlz"
   },
   "source": [
    "## 8 - Training loop(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418,
     "referenced_widgets": [
      "ef1e273f3b2043b192bf81d8f4e124c5",
      "c6d5811378a4442aa4b699e457cadb69",
      "ca676c3ef7ca4d82b99142c3a287ecb7",
      "3b32064c3e2b45f9b67b2eb7c2b2bc93",
      "94113c6f17df4fa5999df7d94523af1e",
      "f41f55ceab4940c8a1533c347a19a4f2",
      "cdc46da27c4444e0b0e6f0fd8388a1b1",
      "88552b181b5f41aa94a1ca52e3bca36d"
     ]
    },
    "colab_type": "code",
    "id": "PgTjjlmxjhlz",
    "outputId": "d1272f7a-1d7e-4ceb-f8ca-68e6748cf0f3"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "bc_train = 367 // batch_size # mini_batch train\n",
    "bc_eval = 101 // batch_size  # mini_batch validation\n",
    "epochs = 100\n",
    "train_writer = SummaryWriter('runs/' + experiment + '/train')\n",
    "val_writer = SummaryWriter('runs/' + experiment + '/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "2UELZ5ghjhl2",
    "outputId": "92120683-83af-44b1-ce71-ad275a5414c1"
   },
   "outputs": [],
   "source": [
    "# Train loop\n",
    "\n",
    "for e in range(1, epochs+1):     \n",
    "    train_loss = 0\n",
    "    print ('-'*15,'Epoch %d' % e, '-'*15)\n",
    "    enet.train()\n",
    "    for _ in tqdm(range(bc_train)):\n",
    "        img, label, inst, dpth = dataiter.next()\n",
    "\n",
    "        # assign data to cpu/gpu\n",
    "        img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n",
    "        label = label.squeeze(1)\n",
    "        inst = inst.squeeze(1)\n",
    "        dpth = dpth.squeeze(1)\n",
    "        \n",
    "        # set non-car labels to 0 for inst\n",
    "#         inst[inst!=26] = 0\n",
    "#         #inst[inst!=15] = 0\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        out = enet(img.float())\n",
    "\n",
    "        # split output into three predictions\n",
    "        label_out, inst_out, dpth_out = out[0], out[1], out[2]\n",
    "    \n",
    "        # get pixel-wise sum for depth\n",
    "        dpth_out = torch.sum(dpth_out, dim=1)\n",
    "\n",
    "        # loss calculation for class segmentation\n",
    "        loss = criterion(label_out, label.long()).float()\n",
    "\n",
    "        # loss calculation for class instance\n",
    "        loss += criterion(inst_out, inst.long()).float()\n",
    "        \n",
    "        #[5] in the below line indicates no. of clusters or no. of instances. Should be taken from the image\n",
    "        #loss += criterion_disc(inst_out, inst.long(), [5] * len(inst)).float()\n",
    "\n",
    "        # loss calculation for depth\n",
    "        loss += criterion_dpth(dpth_out, dpth.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_writer.add_scalar('Loss', train_loss, e)\n",
    "    \n",
    "    if e % eval_every == 0:\n",
    "        with torch.no_grad():\n",
    "            enet.eval()\n",
    "            \n",
    "            eval_loss = 0\n",
    "\n",
    "            # Validation loop\n",
    "            for _ in tqdm(range(bc_eval)):\n",
    "                img, label, inst, dpth = dataiter.next()\n",
    "                img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n",
    "                label = label.squeeze(1)\n",
    "                inst = inst.squeeze(1)\n",
    "                dpth = dpth.squeeze(1)\n",
    "        \n",
    "                out = enet(img.float())\n",
    "                \n",
    "                # split output into three predictions\n",
    "                label_out, inst_out, dpth_out = out[0,:,:,:,:], out[1,:,:,:,:], out[2,:,:,:,:]\n",
    "\n",
    "                # get pixel-wise sum for depth\n",
    "                dpth_out = torch.sum(dpth_out, dim=1)\n",
    "\n",
    "                # loss calculation for class segmentation\n",
    "                eval_loss += criterion(label_out, label.long()).float().item()\n",
    "\n",
    "                # loss calculation for class instance\n",
    "                eval_loss += criterion(inst_out, inst.long()).float().item()\n",
    "                #eval_loss += criterion_disc(inst_out, inst.long(), [5] * len(inst)).float().item()\n",
    "\n",
    "                # loss calculation for depth\n",
    "                eval_loss += criterion_dpth(dpth_out, dpth.float()).item()\n",
    "                \n",
    "            \n",
    "            val_writer.add_scalar('Loss', eval_loss, e)\n",
    "        \n",
    "    if e % print_every == 0:\n",
    "        checkpoint = {\n",
    "            'epochs' : e,\n",
    "            'state_dict' : enet.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, '/content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
    "        print ('Model saved!')\n",
    "    train_writer.flush()\n",
    "    val_writer.flush()\n",
    "\n",
    "print ('Epoch {}/{}...'.format(e, epochs),\n",
    "       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))\n",
    "\n",
    "train_writer.close()\n",
    "val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, _, files in os.walk(folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def discriminative_loss_single(prediction, correct_label, feature_dim, label_shape, \n",
    "                            delta_v, delta_d, param_var, param_dist, param_reg):\n",
    "    \n",
    "    ''' Discriminative loss for a single prediction/label pair.\n",
    "    :param prediction: inference of network\n",
    "    :param correct_label: instance label\n",
    "    :feature_dim: feature dimension of prediction\n",
    "    :param label_shape: shape of label\n",
    "    :param delta_v: cutoff variance distance\n",
    "    :param delta_d: curoff cluster distance\n",
    "    :param param_var: weight for intra cluster variance\n",
    "    :param param_dist: weight for inter cluster distances\n",
    "    :param param_reg: weight regularization\n",
    "    '''\n",
    "\n",
    "    ### Reshape so pixels are aligned along a vector\n",
    "    correct_label = correct_label.reshape([label_shape[1]*label_shape[0]])\n",
    "    print('correct_label')\n",
    "    print(correct_label)\n",
    "    print('\\n')\n",
    "    reshaped_pred = prediction.reshape([label_shape[1]*label_shape[0], feature_dim])\n",
    "    print('reshaped_pred')\n",
    "    print(reshaped_pred)\n",
    "    print('\\n')\n",
    "    ### Count instances\n",
    "    unique_labels, unique_id, counts = correct_label.unique(sorted=False, return_inverse=True, return_counts=True)\n",
    "    #counts = torch.stack([(correct_label==x_u).sum() for x_u in unique_labels])\n",
    "    print('unique_labels')\n",
    "    print(unique_labels)\n",
    "    print('\\n')\n",
    "    print('unique_id')\n",
    "    print(unique_id)\n",
    "    print('\\n')\n",
    "    print('counts')\n",
    "    print(counts)\n",
    "    print('\\n')\n",
    "    counts = counts.float()\n",
    "    print('counts')\n",
    "    print(counts)\n",
    "    print('\\n')\n",
    "    num_instances = torch.tensor(torch.numel(unique_labels))\n",
    "    print('num_instances')\n",
    "    print(num_instances)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    segmented_sum = torch.zeros_like(reshaped_pred).scatter_add_(1, unique_id, reshaped_pred)\n",
    "    \n",
    "    \n",
    "    mu = torch.floor(torch.div(segmented_sum, torch.reshape(counts, (-1, 1))))\n",
    "    mu_expand = torch.gather(mu, unique_id)\n",
    "\n",
    "    ### Calculate l_var\n",
    "    distance = torch.norm(tf.subtract(mu_expand, reshaped_pred), dim=1)\n",
    "    distance = torch.sub(distance, delta_v)\n",
    "    distance = torch.clamp(distance, min=0., max=distance)\n",
    "    distance = torch.mul(distance, distance)\n",
    "\n",
    "    l_var = tf.unsorted_segment_sum(distance, unique_id, num_instances)\n",
    "    l_var = torch.floor(torch.div(l_var, counts))\n",
    "    l_var = l_var.sum()\n",
    "    l_var = torch.divide(l_var, num_instances.float())\n",
    "    \n",
    "    ### Calculate l_dist\n",
    "    \n",
    "    # Get distance for each pair of clusters like this:\n",
    "    #   mu_1 - mu_1\n",
    "    #   mu_2 - mu_1\n",
    "    #   mu_3 - mu_1\n",
    "    #   mu_1 - mu_2\n",
    "    #   mu_2 - mu_2\n",
    "    #   mu_3 - mu_2\n",
    "    #   mu_1 - mu_3\n",
    "    #   mu_2 - mu_3\n",
    "    #   mu_3 - mu_3\n",
    "\n",
    "    mu_interleaved_rep = tf.tile(mu, [num_instances, 1])\n",
    "    mu_band_rep = tf.tile(mu, [1, num_instances])\n",
    "    mu_band_rep = mu_band_rep.reshape([num_instances*num_instances, feature_dim])\n",
    "\n",
    "    mu_diff = torch.sub(mu_band_rep, mu_interleaved_rep)\n",
    "    \n",
    "    # Filter out zeros from same cluster subtraction\n",
    "    intermediate_tensor = torch.sum(tf.abs(mu_diff),axis=1)\n",
    "    zero_vector = torch.zeros(1, dtype=tf.float32)\n",
    "    bool_mask = torch.ne(intermediate_tensor, zero_vector)\n",
    "    mu_diff_bool = tf.torch.masked_select(mu_diff, bool_mask)\n",
    "\n",
    "    mu_norm = torch.norm(mu_diff_bool, p=2, axis=1)\n",
    "    mu_norm = torch.sub(2.*delta_d, mu_norm)\n",
    "    mu_norm = torch.clamp(mu_norm, min=0., max=mu_norm)\n",
    "    mu_norm = torch.mul(mu_norm,mu_norm)\n",
    "\n",
    "    l_dist = torch.mean(mu_norm)\n",
    "\n",
    "    ### Calculate l_reg\n",
    "    l_reg = torch.mean(torch.norm(mu, p=2, dim=1))\n",
    "\n",
    "    param_scale = 1.\n",
    "    l_var = param_var * l_var\n",
    "    l_dist = param_dist * l_dist\n",
    "    l_reg = param_reg * l_reg\n",
    "\n",
    "    loss = param_scale*(l_var + l_dist + l_reg)\n",
    "      \n",
    "    return True\n",
    "    \n",
    "\n",
    "\n",
    "def discriminative_loss(prediction, correct_label, feature_dim, image_shape, \n",
    "                delta_v, delta_d, param_var, param_dist, param_reg):\n",
    "    ''' Iterate over a batch of prediction/label and cumulate loss\n",
    "    :return: discriminative loss and its three components\n",
    "    '''\n",
    "    def cond(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
    "        return tf.less(i, tf.shape(batch)[0])\n",
    "\n",
    "    def body(label, batch, out_loss, out_var, out_dist, out_reg, i):\n",
    "        disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(prediction[i], correct_label[i], feature_dim, image_shape, \n",
    "                        delta_v, delta_d, param_var, param_dist, param_reg)\n",
    "\n",
    "        out_loss = out_loss.write(i, disc_loss)\n",
    "        out_var = out_var.write(i, l_var)\n",
    "        out_dist = out_dist.write(i, l_dist)\n",
    "        out_reg = out_reg.write(i, l_reg)\n",
    "\n",
    "        return label, batch, out_loss, out_var, out_dist, out_reg, i + 1\n",
    "\n",
    "    # TensorArray is a data structure that support dynamic writing\n",
    "    output_ta_loss = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_var = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_dist = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "    output_ta_reg = tf.TensorArray(dtype=tf.float32,\n",
    "                   size=0,\n",
    "                   dynamic_size=True)\n",
    "\n",
    "    _, _, out_loss_op, out_var_op, out_dist_op, out_reg_op, _  = tf.while_loop(cond, body, [correct_label, \n",
    "                                                        prediction, \n",
    "                                                        output_ta_loss, \n",
    "                                                        output_ta_var, \n",
    "                                                        output_ta_dist, \n",
    "                                                        output_ta_reg, \n",
    "                                                        0])\n",
    "    out_loss_op = out_loss_op.stack()\n",
    "    out_var_op = out_var_op.stack()\n",
    "    out_dist_op = out_dist_op.stack()\n",
    "    out_reg_op = out_reg_op.stack()\n",
    "    \n",
    "    disc_loss = tf.reduce_mean(out_loss_op)\n",
    "    l_var = tf.reduce_mean(out_var_op)\n",
    "    l_dist = tf.reduce_mean(out_dist_op)\n",
    "    l_reg = tf.reduce_mean(out_reg_op)\n",
    "\n",
    "    return disc_loss, l_var, l_dist, l_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.tensor([[1,2061],[2,2062]])\n",
    "correct_label = torch.tensor([[1,2061],[2,2062]])\n",
    "disc_loss, l_var, l_dist, l_reg = discriminative_loss_single(prediction, correct_label, 1, (2,2), \n",
    "                            1, 1, 0.001, 0.5, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
