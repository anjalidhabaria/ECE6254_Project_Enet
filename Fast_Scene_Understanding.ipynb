{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wrqIinejhkt"
   },
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import future\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-dj3wA0jhk5"
   },
   "source": [
    "## Define the ENet model\n",
    "\n",
    "We decided to model following residual blocks as separate class to model ENET encoder and decoder:\n",
    "    - Initial block\n",
    "    - RDDNeck - class for regular, downsampling and dilated bottlenecks\n",
    "    - ASNeck - class for asymetric bottlenecks\n",
    "    - UBNeck - class for upsampling bottlenecks\n",
    "\n",
    "ENET architecture is autoencoder based model and is divided into 5 sub-blocks. Pleas refer [ENET paper](https://arxiv.org/pdf/1606.02147.pdf) for details of each sub-block. ENET building blocks code is taken from [here](https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation).\n",
    "\n",
    "Fast scene understanding uses first 2 sub-blocks as encoder and remaining 3 as decoder. In this implemantation, there is 1 shared encoder and 3 separate decoder for 3 tasks(instance segementation, semantic segmentation, Depth estimation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lin/ECE6254_Project_Enet\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "nb_dir = os.getcwd()\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "print(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ENetDecoder import ENetDecoder\n",
    "from models.ENetEncoder import ENetEncoder\n",
    "\n",
    "class BranchedENet(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        self.enc = ENetEncoder(C)\n",
    "        \n",
    "        self.dec1 = ENetDecoder(C)\n",
    "        self.dec2 = ENetDecoder(C)\n",
    "        self.dec3 = ENetDecoder(C)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Output of Encoder\n",
    "        x, i1, i2 = self.enc(x)\n",
    "        # output of all 3 decoder in list\n",
    "        #x = torch.stack([self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2)])\n",
    "        x = (self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnRAXNvbjhlf"
   },
   "source": [
    "## Instantiate the ENet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Zo6pIjqjhlg"
   },
   "outputs": [],
   "source": [
    "enet = BranchedENet(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQAvwXDpjhlj"
   },
   "outputs": [],
   "source": [
    "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enet = enet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bNQ-SAqjhky"
   },
   "source": [
    "## Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cityscapes import Cityscapes as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 1024\n",
    "dataset_dir = 'data/cityscape'\n",
    "image_transform = transforms.Compose(\n",
    "        [transforms.Resize((height,width)),transforms.ToTensor()])\n",
    "train_set = dataset(dataset_dir,transform=image_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set,batch_size=1,shuffle=True,\n",
    "        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bremen/bremen_000282_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000282_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000282_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bremen/bremen_000282_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/erfurt/erfurt_000022_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000022_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000022_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/erfurt/erfurt_000022_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bremen/bremen_000267_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000267_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000267_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bremen/bremen_000267_000019_disparity.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin/.local/lib/python3.6/site-packages/torch/jit/__init__.py:1044: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 19, 3, 489] (0.019528158009052277 vs. 0.009444912895560265) and 91505 other locations (0.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n",
      "/home/lin/.local/lib/python3.6/site-packages/torch/jit/__init__.py:1044: TracerWarning: Output nr 2. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 27, 24, 504] (-0.0011107708560302854 vs. 0.0064564417116343975) and 77003 other locations (0.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n",
      "/home/lin/.local/lib/python3.6/site-packages/torch/jit/__init__.py:1044: TracerWarning: Output nr 3. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 18, 55, 505] (0.015072399750351906 vs. 0.007174754049628973) and 86947 other locations (0.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/erfurt/erfurt_000077_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000077_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000077_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/erfurt/erfurt_000077_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/strasbourg/strasbourg_000000_004112_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/strasbourg/strasbourg_000000_004112_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/strasbourg/strasbourg_000000_004112_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/strasbourg/strasbourg_000000_004112_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/jena/jena_000010_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/jena/jena_000010_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/jena/jena_000010_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/jena/jena_000010_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/aachen/aachen_000038_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/aachen/aachen_000038_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/aachen/aachen_000038_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/aachen/aachen_000038_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bochum/bochum_000000_033056_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bochum/bochum_000000_033056_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bochum/bochum_000000_033056_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bochum/bochum_000000_033056_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/aachen/aachen_000075_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/aachen/aachen_000075_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/aachen/aachen_000075_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/aachen/aachen_000075_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/hanover/hanover_000000_027650_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_027650_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_027650_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/hanover/hanover_000000_027650_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/hanover/hanover_000000_039021_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_039021_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_039021_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/hanover/hanover_000000_039021_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bremen/bremen_000247_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000247_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000247_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bremen/bremen_000247_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/erfurt/erfurt_000003_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000003_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/erfurt/erfurt_000003_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/erfurt/erfurt_000003_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/cologne/cologne_000039_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/cologne/cologne_000039_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/cologne/cologne_000039_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/cologne/cologne_000039_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/hanover/hanover_000000_046398_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_046398_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/hanover/hanover_000000_046398_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/hanover/hanover_000000_046398_disparity.png\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "img, label, inst, dpth = dataiter.next()\n",
    "\n",
    "writer.add_graph(enet, img.to(device))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to add in training loop\n",
    "# writer.add_scalar('training loss',running_loss / 1000,epoch * len(trainloader) + i)\n",
    "\n",
    "# for n_iter in range(100):\n",
    "#     writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpCP5JKAjhlW"
   },
   "source": [
    "## 3 - Losses(todo)\n",
    "(1) Semantic Segmentation Loss\n",
    "\n",
    "(2) Instantance Segmentation Loss\n",
    "\n",
    "(3) Depth Estimation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "0b6IvyXIjhlW",
    "outputId": "ace7c895-c043-4152-d5a5-e9339ebf6854"
   },
   "outputs": [],
   "source": [
    "# def compute_instance_cost(a_C, a_G):\n",
    "#     M = {}\n",
    "#     tools = require 'tools/tools'\n",
    "\n",
    "#     in_margin = 0.5\n",
    "#     out_margin = 1.5\n",
    "#     Lnorm = 2\n",
    "\n",
    "#     function norm(inp, L)\n",
    "#         n\n",
    "#         if (L == 1) then\n",
    "#             n = torch.sum(torch.abs(inp), 1)\n",
    "#         else\n",
    "#             n = torch.sqrt(torch.sum(torch.pow(inp, 2), 1) + 1e-8)\n",
    "#         end\n",
    "#         return n\n",
    "#     end\n",
    "\n",
    "#     -- prediction: batchsize x nDim x h x w\n",
    "#     -- labels: batchsize x classes x h x w\n",
    "\n",
    "#     local lossf =\n",
    "#         function(prediction, labels)\n",
    "#         local batchsize = prediction:size(1)\n",
    "#         local c = prediction:size(2)\n",
    "#         local height = prediction:size(3)\n",
    "#         local width = prediction:size(4)\n",
    "#         local nInstanceMaps = labels:size(2)\n",
    "#         local loss = 0\n",
    "\n",
    "#         M.loss_dist = 0\n",
    "#         M.loss_var = 0\n",
    "\n",
    "#         for b = 1, batchsize do\n",
    "#             local pred = prediction[b] -- c x h x w\n",
    "#             local loss_var = 0\n",
    "#             local loss_dist = 0\n",
    "\n",
    "#             for h = 1, nInstanceMaps do\n",
    "#                 local label = labels[b][h]:view(1, height, width) -- 1 x h x w\n",
    "#                 local means = {}\n",
    "#                 local loss_v = 0\n",
    "#                 local loss_d = 0\n",
    "\n",
    "#                 -- center pull force\n",
    "#                 for j = 1, label:max() do\n",
    "#                     local mask = label:eq(j)\n",
    "#                     local mask_sum = mask:sum()\n",
    "#                     if (mask_sum > 1) then\n",
    "#                         local inst = pred[mask:expandAs(pred)]:view(c, -1, 1) -- c x -1 x 1\n",
    "\n",
    "#                         -- Calculate mean of instance\n",
    "#                         local mean = torch.mean(inst, 2) -- c x 1 x 1\n",
    "#                         table.insert(means, mean)\n",
    "\n",
    "#                         -- Calculate variance of instance\n",
    "#                         local var = norm((inst - mean:expandAs(inst)), 2) -- 1 x -1 x 1\n",
    "#                         var = torch.cmax(var - (in_margin), 0)\n",
    "#                         local not_hinged = torch.sum(torch.gt(var, 0))\n",
    "\n",
    "#                         var = torch.pow(var, 2)\n",
    "#                         var = var:view(-1)\n",
    "\n",
    "#                         var = torch.mean(var)\n",
    "#                         loss_v = loss_v + var\n",
    "#                     end\n",
    "#                 end\n",
    "\n",
    "#                 loss_var = loss_var + loss_v\n",
    "\n",
    "#                 -- center push force\n",
    "#                 if (#means > 1) then\n",
    "#                     for j = 1, #means do\n",
    "#                         local mean_A = means[j] -- c x 1 x 1\n",
    "#                         for k = j + 1, #means do\n",
    "#                             local mean_B = means[k] -- c x 1 x 1\n",
    "#                             local d = norm(mean_A - mean_B, Lnorm) -- 1 x 1 x 1\n",
    "#                             d = torch.pow(torch.cmax(-(d - 2 * out_margin), 0), 2)\n",
    "#                             loss_d = loss_d + d[1][1][1]\n",
    "#                         end\n",
    "#                     end\n",
    "\n",
    "#                     loss_dist = loss_dist + loss_d / ((#means - 1) + 1e-8)\n",
    "#                 end\n",
    "#             end\n",
    "\n",
    "#             loss = loss + (loss_dist + loss_var)\n",
    "#         end\n",
    "\n",
    "#         loss = loss / batchsize + torch.sum(prediction) * 0\n",
    "\n",
    "#         return loss\n",
    "#     end\n",
    "\n",
    "# return lossf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "4Dv3_I_Ljhla",
    "outputId": "a4a00785-0446-46aa-edd0-6900a1d1a78d"
   },
   "outputs": [],
   "source": [
    "def inverse_huber_loss(out, target):\n",
    "    absdiff = torch.abs(out-target)\n",
    "    C = 0.2*torch.max(absdiff)\n",
    "    return torch.mean(torch.where(absdiff<C, absdiff, (absdiff*absdiff+C*C)/(2*C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "lHm_GLNBjhld",
    "outputId": "3962711f-0616-4f3b-9570-426ec4902f9e"
   },
   "outputs": [],
   "source": [
    "# local grad = require 'autograd'\n",
    "\n",
    "# def lossfunction(lossf_name, weights):\n",
    "#     if (lossf_name == 'softmaxLoss') then\n",
    "#         lossfunction = cudnn.SpatialCrossEntropyCriterion(weights)\n",
    "#     elseif (lossf_name == 'huberLoss') then\n",
    "#         lossfunction = grad.nn.AutoCriterion('depthLoss_huber')(require 'lossf/myHuberLoss')\n",
    "#     elseif (lossf_name == 'instanceLoss') then\n",
    "#         lossfunction = grad.nn.AutoCriterion('instance_loss')(require 'lossf/myInstanceLoss')\n",
    "#     else\n",
    "#         assert(false, 'Cannot load lossfunction ' .. opts.lossf)\n",
    "#     end\n",
    "\n",
    "#     return lossfunction\n",
    "# end\n",
    "\n",
    "# return getLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxnljE48jhlm"
   },
   "source": [
    "# Step 5 and 6 has been done in dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlATaBtjjhlw"
   },
   "source": [
    "## 7 - Define the Hyperparameters(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDZLtYRbjhlw"
   },
   "outputs": [],
   "source": [
    "from data.utils import enet_weighing\n",
    "lr = 5e-4\n",
    "batch_size = 1\n",
    "\n",
    "# figure out enet_weighing issue\n",
    "#criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(enet_weighing(train_loader, 12)).to(device))\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion_dpth = torch.nn.MSELoss(reduction='mean').to(device)\n",
    "optimizer = torch.optim.Adam(enet.parameters(), \n",
    "                             lr=lr,\n",
    "                             weight_decay=2e-4)\n",
    "\n",
    "print_every = 5\n",
    "eval_every = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Odil0dEjhlz"
   },
   "source": [
    "## 8 - Training loop(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418,
     "referenced_widgets": [
      "ef1e273f3b2043b192bf81d8f4e124c5",
      "c6d5811378a4442aa4b699e457cadb69",
      "ca676c3ef7ca4d82b99142c3a287ecb7",
      "3b32064c3e2b45f9b67b2eb7c2b2bc93",
      "94113c6f17df4fa5999df7d94523af1e",
      "f41f55ceab4940c8a1533c347a19a4f2",
      "cdc46da27c4444e0b0e6f0fd8388a1b1",
      "88552b181b5f41aa94a1ca52e3bca36d"
     ]
    },
    "colab_type": "code",
    "id": "PgTjjlmxjhlz",
    "outputId": "d1272f7a-1d7e-4ceb-f8ca-68e6748cf0f3"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "bc_train = 367 // batch_size # mini_batch train\n",
    "bc_eval = 101 // batch_size  # mini_batch validation\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "2UELZ5ghjhl2",
    "outputId": "92120683-83af-44b1-ce71-ad275a5414c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e1b500182641008ff5ab1f72463d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=367.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8231cef50e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# split output into three predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5f3c4e67e783>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# output of all 3 decoder in list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#x = torch.stack([self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ECE6254_Project_Enet/models/ENetDecoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, i1, i2)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# The fifth bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb51\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ECE6254_Project_Enet/models/UBNeck.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, indices)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvt3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    776\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    777\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    print ('-'*15,'Epoch %d' % e, '-'*15)\n",
    "    \n",
    "    enet.train()\n",
    "    \n",
    "    for _ in tqdm(range(bc_train)):\n",
    "        img, label, inst, dpth = dataiter.next()\n",
    "\n",
    "        # assign data to cpu/gpu\n",
    "        img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n",
    "        label = label.squeeze(1)\n",
    "        inst = inst.squeeze(1)\n",
    "        dpth = dpth.squeeze(1)\n",
    "        \n",
    "        # set non-car labels to 0 for inst\n",
    "        inst[inst!=26] = 0\n",
    "        #inst[inst!=15] = 0\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = enet(img.float())\n",
    "\n",
    "        # split output into three predictions\n",
    "        label_out, inst_out, dpth_out = out[0], out[1], out[2]\n",
    "    \n",
    "        # get pixel-wise sum for depth\n",
    "        dpth_out = torch.sum(dpth_out, dim=1)\n",
    "\n",
    "        # loss calculation for class segmentation\n",
    "        loss = criterion(label_out, label.long()).float()\n",
    "\n",
    "        # loss calculation for class instance\n",
    "        loss += criterion(inst_out, inst.long()).float()\n",
    "\n",
    "        # loss calculation for depth\n",
    "        loss += criterion_dpth(dpth_out, dpth.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    writer.add_scalar('Loss/train', train_loss, e)\n",
    "    \n",
    "    if e % eval_every == 0:\n",
    "        with torch.no_grad():\n",
    "            enet.eval()\n",
    "            \n",
    "            eval_loss = 0\n",
    "\n",
    "            # Validation loop\n",
    "            for _ in tqdm(range(bc_eval)):\n",
    "                img, label, inst, dpth = dataiter.next()\n",
    "                img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n",
    "                label = label.squeeze(1)\n",
    "                inst = inst.squeeze(1)\n",
    "                dpth = dpth.squeeze(1)\n",
    "        \n",
    "                out = enet(img.float())\n",
    "                \n",
    "                # split output into three predictions\n",
    "                label_out, inst_out, dpth_out = out[0,:,:,:,:], out[1,:,:,:,:], out[2,:,:,:,:]\n",
    "\n",
    "                # get pixel-wise sum for depth\n",
    "                dpth_out = torch.sum(dpth_out, dim=1)\n",
    "\n",
    "                # loss calculation for class segmentation\n",
    "                eval_loss += criterion(label_out, label.long()).float().item()\n",
    "\n",
    "                # loss calculation for class instance\n",
    "                eval_loss += criterion(inst_out, inst.long()).float().item()\n",
    "\n",
    "                # loss calculation for depth\n",
    "                eval_loss += criterion_dpth(dpth_out, dpth.float()).item()\n",
    "                \n",
    "            \n",
    "            writer.add_scalar('Loss/test', eval_loss, e // eval_every)\n",
    "        \n",
    "    if e % print_every == 0:\n",
    "        checkpoint = {\n",
    "            'epochs' : e,\n",
    "            'state_dict' : enet.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, '/content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
    "        print ('Model saved!')\n",
    "\n",
    "print ('Epoch {}/{}...'.format(e, epochs),\n",
    "       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, _, files in os.walk(folder):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
