{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wrqIinejhkt"
   },
   "outputs": [],
   "source": [
    "import zipfile as zf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import future\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-dj3wA0jhk5"
   },
   "source": [
    "## Define the ENet model\n",
    "\n",
    "We decided to model following residual blocks as separate class to model ENET encoder and decoder:\n",
    "    - Initial block\n",
    "    - RDDNeck - class for regular, downsampling and dilated bottlenecks\n",
    "    - ASNeck - class for asymetric bottlenecks\n",
    "    - UBNeck - class for upsampling bottlenecks\n",
    "\n",
    "ENET architecture is autoencoder based model and is divided into 5 sub-blocks. Pleas refer [ENET paper](https://arxiv.org/pdf/1606.02147.pdf) for details of each sub-block. ENET building blocks code is taken from [here](https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation).\n",
    "\n",
    "Fast scene understanding uses first 2 sub-blocks as encoder and remaining 3 as decoder. In this implemantation, there is 1 shared encoder and 3 separate decoder for 3 tasks(instance segementation, semantic segmentation, Depth estimation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lin/ECE6254_Project_Enet\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "nb_dir = os.getcwd()\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "print(nb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ENetDecoder import ENetDecoder\n",
    "from models.ENetEncoder import ENetEncoder\n",
    "\n",
    "class BranchedENet(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        self.enc = ENetEncoder(C)\n",
    "        \n",
    "        self.dec1 = ENetDecoder(C)\n",
    "        self.dec2 = ENetDecoder(C)\n",
    "        self.dec3 = ENetDecoder(C)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Output of Encoder\n",
    "        x, i1, i2 = self.enc(x)\n",
    "        # output of all 3 decoder in list\n",
    "        x = torch.stack([self.dec1(x, i1, i2), self.dec2(x, i1, i2), self.dec3(x, i1, i2)])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnRAXNvbjhlf"
   },
   "source": [
    "## Instantiate the ENet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Zo6pIjqjhlg"
   },
   "outputs": [],
   "source": [
    "enet = BranchedENet(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQAvwXDpjhlj"
   },
   "outputs": [],
   "source": [
    "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "enet = enet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bNQ-SAqjhky"
   },
   "source": [
    "## Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.cityscapes import Cityscapes as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 1024\n",
    "dataset_dir = 'data/cityscape'\n",
    "image_transform = transforms.Compose(\n",
    "        [transforms.Resize((height,width)),transforms.ToTensor()])\n",
    "train_set = dataset(dataset_dir,transform=image_transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set,batch_size=1,shuffle=True,\n",
    "        num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/monchengladbach/monchengladbach_000000_010733_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/monchengladbach/monchengladbach_000000_010733_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/monchengladbach/monchengladbach_000000_010733_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/monchengladbach/monchengladbach_000000_010733_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/strasbourg/strasbourg_000001_011775_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/strasbourg/strasbourg_000001_011775_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/strasbourg/strasbourg_000001_011775_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/strasbourg/strasbourg_000001_011775_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/zurich/zurich_000040_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/zurich/zurich_000040_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/zurich/zurich_000040_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/zurich/zurich_000040_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bochum/bochum_000000_011255_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bochum/bochum_000000_011255_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bochum/bochum_000000_011255_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bochum/bochum_000000_011255_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/weimar/weimar_000134_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/weimar/weimar_000134_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/weimar/weimar_000134_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/weimar/weimar_000134_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/hamburg/hamburg_000000_023472_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/hamburg/hamburg_000000_023472_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/hamburg/hamburg_000000_023472_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/hamburg/hamburg_000000_023472_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/tubingen/tubingen_000120_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/tubingen/tubingen_000120_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/tubingen/tubingen_000120_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/tubingen/tubingen_000120_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bremen/bremen_000273_000019_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000273_000019_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bremen/bremen_000273_000019_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bremen/bremen_000273_000019_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/strasbourg/strasbourg_000000_025491_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/strasbourg/strasbourg_000000_025491_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/strasbourg/strasbourg_000000_025491_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/strasbourg/strasbourg_000000_025491_disparity.png\n",
      "data/cityscape/leftImg8bit_trainvaltest/leftImg8bit/train/bochum/bochum_000000_006026_leftImg8bit.png data/cityscape/gtFine_trainvaltest/gtFine/train/bochum/bochum_000000_006026_gtFine_labelIds.png data/cityscape/gtFine_trainvaltest/gtFine/train/bochum/bochum_000000_006026_gtFine_instanceIds.png data/cityscape/disparity_trainvaltest/disparity/train/bochum/bochum_000000_006026_disparity.png\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "img, label, inst, dpth = dataiter.next()\n",
    "\n",
    "writer.add_graph(enet, img.to(device))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to add in training loop\n",
    "# writer.add_scalar('training loss',running_loss / 1000,epoch * len(trainloader) + i)\n",
    "\n",
    "# for n_iter in range(100):\n",
    "#     writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "#     writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpCP5JKAjhlW"
   },
   "source": [
    "## 3 - Losses(todo)\n",
    "(1) Semantic Segmentation Loss\n",
    "\n",
    "(2) Instantance Segmentation Loss\n",
    "\n",
    "(3) Depth Estimation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "0b6IvyXIjhlW",
    "outputId": "ace7c895-c043-4152-d5a5-e9339ebf6854"
   },
   "outputs": [],
   "source": [
    "# def compute_instance_cost(a_C, a_G):\n",
    "#     M = {}\n",
    "#     tools = require 'tools/tools'\n",
    "\n",
    "#     in_margin = 0.5\n",
    "#     out_margin = 1.5\n",
    "#     Lnorm = 2\n",
    "\n",
    "#     function norm(inp, L)\n",
    "#         n\n",
    "#         if (L == 1) then\n",
    "#             n = torch.sum(torch.abs(inp), 1)\n",
    "#         else\n",
    "#             n = torch.sqrt(torch.sum(torch.pow(inp, 2), 1) + 1e-8)\n",
    "#         end\n",
    "#         return n\n",
    "#     end\n",
    "\n",
    "#     -- prediction: batchsize x nDim x h x w\n",
    "#     -- labels: batchsize x classes x h x w\n",
    "\n",
    "#     local lossf =\n",
    "#         function(prediction, labels)\n",
    "#         local batchsize = prediction:size(1)\n",
    "#         local c = prediction:size(2)\n",
    "#         local height = prediction:size(3)\n",
    "#         local width = prediction:size(4)\n",
    "#         local nInstanceMaps = labels:size(2)\n",
    "#         local loss = 0\n",
    "\n",
    "#         M.loss_dist = 0\n",
    "#         M.loss_var = 0\n",
    "\n",
    "#         for b = 1, batchsize do\n",
    "#             local pred = prediction[b] -- c x h x w\n",
    "#             local loss_var = 0\n",
    "#             local loss_dist = 0\n",
    "\n",
    "#             for h = 1, nInstanceMaps do\n",
    "#                 local label = labels[b][h]:view(1, height, width) -- 1 x h x w\n",
    "#                 local means = {}\n",
    "#                 local loss_v = 0\n",
    "#                 local loss_d = 0\n",
    "\n",
    "#                 -- center pull force\n",
    "#                 for j = 1, label:max() do\n",
    "#                     local mask = label:eq(j)\n",
    "#                     local mask_sum = mask:sum()\n",
    "#                     if (mask_sum > 1) then\n",
    "#                         local inst = pred[mask:expandAs(pred)]:view(c, -1, 1) -- c x -1 x 1\n",
    "\n",
    "#                         -- Calculate mean of instance\n",
    "#                         local mean = torch.mean(inst, 2) -- c x 1 x 1\n",
    "#                         table.insert(means, mean)\n",
    "\n",
    "#                         -- Calculate variance of instance\n",
    "#                         local var = norm((inst - mean:expandAs(inst)), 2) -- 1 x -1 x 1\n",
    "#                         var = torch.cmax(var - (in_margin), 0)\n",
    "#                         local not_hinged = torch.sum(torch.gt(var, 0))\n",
    "\n",
    "#                         var = torch.pow(var, 2)\n",
    "#                         var = var:view(-1)\n",
    "\n",
    "#                         var = torch.mean(var)\n",
    "#                         loss_v = loss_v + var\n",
    "#                     end\n",
    "#                 end\n",
    "\n",
    "#                 loss_var = loss_var + loss_v\n",
    "\n",
    "#                 -- center push force\n",
    "#                 if (#means > 1) then\n",
    "#                     for j = 1, #means do\n",
    "#                         local mean_A = means[j] -- c x 1 x 1\n",
    "#                         for k = j + 1, #means do\n",
    "#                             local mean_B = means[k] -- c x 1 x 1\n",
    "#                             local d = norm(mean_A - mean_B, Lnorm) -- 1 x 1 x 1\n",
    "#                             d = torch.pow(torch.cmax(-(d - 2 * out_margin), 0), 2)\n",
    "#                             loss_d = loss_d + d[1][1][1]\n",
    "#                         end\n",
    "#                     end\n",
    "\n",
    "#                     loss_dist = loss_dist + loss_d / ((#means - 1) + 1e-8)\n",
    "#                 end\n",
    "#             end\n",
    "\n",
    "#             loss = loss + (loss_dist + loss_var)\n",
    "#         end\n",
    "\n",
    "#         loss = loss / batchsize + torch.sum(prediction) * 0\n",
    "\n",
    "#         return loss\n",
    "#     end\n",
    "\n",
    "# return lossf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "4Dv3_I_Ljhla",
    "outputId": "a4a00785-0446-46aa-edd0-6900a1d1a78d"
   },
   "outputs": [],
   "source": [
    "def inverse_huber_loss(out, target):\n",
    "    absdiff = torch.abs(out-target); print(absdiff)\n",
    "    C = 0.2*torch.max(absdiff); print(C)\n",
    "    return torch.mean(torch.where(absdiff<C, absdiff, (absdiff*absdiff+C*C)/(2*C+0.0001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "lHm_GLNBjhld",
    "outputId": "3962711f-0616-4f3b-9570-426ec4902f9e"
   },
   "outputs": [],
   "source": [
    "# local grad = require 'autograd'\n",
    "\n",
    "# def lossfunction(lossf_name, weights):\n",
    "#     if (lossf_name == 'softmaxLoss') then\n",
    "#         lossfunction = cudnn.SpatialCrossEntropyCriterion(weights)\n",
    "#     elseif (lossf_name == 'huberLoss') then\n",
    "#         lossfunction = grad.nn.AutoCriterion('depthLoss_huber')(require 'lossf/myHuberLoss')\n",
    "#     elseif (lossf_name == 'instanceLoss') then\n",
    "#         lossfunction = grad.nn.AutoCriterion('instance_loss')(require 'lossf/myInstanceLoss')\n",
    "#     else\n",
    "#         assert(false, 'Cannot load lossfunction ' .. opts.lossf)\n",
    "#     end\n",
    "\n",
    "#     return lossfunction\n",
    "# end\n",
    "\n",
    "# return getLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxnljE48jhlm"
   },
   "source": [
    "# Step 5 and 6 has been done in dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlATaBtjjhlw"
   },
   "source": [
    "## 7 - Define the Hyperparameters(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDZLtYRbjhlw"
   },
   "outputs": [],
   "source": [
    "from data.utils import enet_weighing\n",
    "lr = 5e-4\n",
    "batch_size = 1\n",
    "\n",
    "# figure out enet_weighing issue\n",
    "#criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(enet_weighing(train_loader, 12)).to(device))\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion_dpth = torch.nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(enet.parameters(), \n",
    "                             lr=lr,\n",
    "                             weight_decay=2e-4)\n",
    "\n",
    "print_every = 5\n",
    "eval_every = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Odil0dEjhlz"
   },
   "source": [
    "## 8 - Training loop(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418,
     "referenced_widgets": [
      "ef1e273f3b2043b192bf81d8f4e124c5",
      "c6d5811378a4442aa4b699e457cadb69",
      "ca676c3ef7ca4d82b99142c3a287ecb7",
      "3b32064c3e2b45f9b67b2eb7c2b2bc93",
      "94113c6f17df4fa5999df7d94523af1e",
      "f41f55ceab4940c8a1533c347a19a4f2",
      "cdc46da27c4444e0b0e6f0fd8388a1b1",
      "88552b181b5f41aa94a1ca52e3bca36d"
     ]
    },
    "colab_type": "code",
    "id": "PgTjjlmxjhlz",
    "outputId": "d1272f7a-1d7e-4ceb-f8ca-68e6748cf0f3"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "bc_train = 367 // batch_size # mini_batch train\n",
    "bc_eval = 101 // batch_size  # mini_batch validation\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "2UELZ5ghjhl2",
    "outputId": "92120683-83af-44b1-ce71-ad275a5414c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3074c8c77da44a6d946634f2fb09fd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=367.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "Count\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-82df4f79a2e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# loss calculation for depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion_dpth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpth_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    print ('-'*15,'Epoch %d' % e, '-'*15)\n",
    "    \n",
    "    enet.train()\n",
    "    \n",
    "    for _ in tqdm(range(bc_train)):\n",
    "        img, label, inst, dpth = dataiter.next()\n",
    "        print(\"Count\")\n",
    "        # assign data to cpu/gpu\n",
    "        img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n",
    "        label = label.squeeze(1)\n",
    "        inst = inst.squeeze(1)\n",
    "        dpth = dpth.squeeze(1)\n",
    "        \n",
    "        # set non-car labels to 0 for inst\n",
    "        inst[inst!=26] = 0\n",
    "        #inst[inst!=15] = 0\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = enet(img.float())\n",
    "\n",
    "        # split output into three predictions\n",
    "        label_out, inst_out, dpth_out = out[0,:,:,:,:], out[1,:,:,:,:], out[2,:,:,:,:]\n",
    "    \n",
    "        # get pixel-wise sum for depth\n",
    "        dpth_out = torch.sum(dpth_out, dim=1)\n",
    "\n",
    "        # loss calculation for class segmentation\n",
    "        loss = criterion(label_out, label.long()).float()\n",
    "\n",
    "        # loss calculation for class instance\n",
    "        loss += criterion(inst_out, inst.long()).float()\n",
    "\n",
    "        # loss calculation for depth\n",
    "        loss += criterion_dpth(dpth_out, dpth.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    writer.add_scalar('Loss/train', train_loss, e)\n",
    "    \n",
    "    if e % eval_every == 0:\n",
    "        with torch.no_grad():\n",
    "            enet.eval()\n",
    "            \n",
    "            eval_loss = 0\n",
    "\n",
    "            # Validation loop\n",
    "            for _ in tqdm(range(bc_eval)):\n",
    "                img, label, inst, dpth = dataiter.next()\n",
    "                img, label, inst, dpth = img.to(device), label.to(device), inst.to(device), dpth.to(device)\n",
    "                label = label.squeeze(1)\n",
    "                inst = inst.squeeze(1)\n",
    "                dpth = dpth.squeeze(1)\n",
    "        \n",
    "                out = enet(img.float())\n",
    "                \n",
    "                # split output into three predictions\n",
    "                label_out, inst_out, dpth_out = out[0,:,:,:,:], out[1,:,:,:,:], out[2,:,:,:,:]\n",
    "\n",
    "                # get pixel-wise sum for depth\n",
    "                dpth_out = torch.sum(dpth_out, dim=1)\n",
    "\n",
    "                # loss calculation for class segmentation\n",
    "                eval_loss += criterion(label_out, label.long()).float().item()\n",
    "\n",
    "                # loss calculation for class instance\n",
    "                eval_loss += criterion(inst_out, inst.long()).float().item()\n",
    "\n",
    "                # loss calculation for depth\n",
    "                eval_loss += criterion_dpth(dpth_out, dpth.float()).item()\n",
    "                \n",
    "            \n",
    "            writer.add_scalar('Loss/test', eval_loss, e // eval_every)\n",
    "        \n",
    "    if e % print_every == 0:\n",
    "        checkpoint = {\n",
    "            'epochs' : e,\n",
    "            'state_dict' : enet.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, '/content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
    "        print ('Model saved!')\n",
    "\n",
    "print ('Epoch {}/{}...'.format(e, epochs),\n",
    "       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnJmyQw9b1jm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, _, files in os.walk(folder):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
