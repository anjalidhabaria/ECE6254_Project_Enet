{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/pxcz2wdz04zxocq/CamVid.zip?dl=1 -O CamVid.zip\n",
    "#!unzip CamVid.zip\n",
    "with ZipFile('CamVid.zip', 'r') as zf:\n",
    "    zf.extractall('content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip CamVid.zip -d images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Create the ENet model\n",
    "Major ENet code is borrowed from iArunava/ENet-Real-Time-Semantic-Segmentation\n",
    "We decided to to split the model to three sub classes:\n",
    "\n",
    "1) Initial block\n",
    "\n",
    "2) RDDNeck - class for regular, downsampling and dilated bottlenecks\n",
    "\n",
    "3) ASNeck - class for asymetric bottlenecks\n",
    "\n",
    "4) UBNeck - class for upsampling bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialBlock(nn.Module):\n",
    "  \n",
    "  # Initial block of the model:\n",
    "  #         Input\n",
    "  #        /     \\\n",
    "  #       /       \\\n",
    "  #maxpool2d    conv2d-3x3\n",
    "  #       \\       /  \n",
    "  #        \\     /\n",
    "  #      concatenate\n",
    "   \n",
    "    def __init__ (self,in_channels = 3,out_channels = 13):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, \n",
    "                                      stride = 2, \n",
    "                                      padding = 0)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, \n",
    "                                out_channels,\n",
    "                                kernel_size = 3,\n",
    "                                stride = 2, \n",
    "                                padding = 1)\n",
    "\n",
    "        self.prelu = nn.PReLU(16)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        main = self.conv(x)\n",
    "        main = self.batchnorm(main)\n",
    "        \n",
    "        side = self.maxpool(x)\n",
    "        \n",
    "        # concatenating on the channels axis\n",
    "        x = torch.cat((main, side), dim=1)\n",
    "        x = self.prelu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDDNeck(nn.Module):\n",
    "    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n",
    "      \n",
    "  # Regular|Dilated|Downsampling bottlenecks:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # maxpooling2d   conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params: \n",
    "  #  dilation (bool) - if True: creating dilation bottleneck\n",
    "  #  down_flag (bool) - if True: creating downsampling bottleneck\n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  #  p - dropout ratio\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = dilation\n",
    "        self.down_flag = down_flag\n",
    "        \n",
    "        # calculating the number of reduced channels\n",
    "        if down_flag:\n",
    "            self.stride = 2\n",
    "            self.reduced_depth = int(in_channels // projection_ratio)\n",
    "        else:\n",
    "            self.stride = 1\n",
    "            self.reduced_depth = int(out_channels // projection_ratio)\n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n",
    "                                      stride = 2,\n",
    "                                      padding = 0, return_indices=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=p)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False,\n",
    "                               dilation = 1)\n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = self.stride,\n",
    "                                  padding = self.dilation,\n",
    "                                  bias = True,\n",
    "                                  dilation = self.dilation)\n",
    "                                  \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False,\n",
    "                                  dilation = 1)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        if self.down_flag:\n",
    "            x_copy, indices = self.maxpool(x_copy)\n",
    "          \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            if torch.cuda.is_available():\n",
    "                extras = extras.cuda()\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "\n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        if self.down_flag:\n",
    "            return x, indices\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, projection_ratio=4):\n",
    "      \n",
    "  # Asymetric bottleneck:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x5\n",
    "  #      |             |\n",
    "  #      |         conv2d-5x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params:    \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (1, 5),\n",
    "                                  stride = 1,\n",
    "                                  padding = (0, 2),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (5, 1),\n",
    "                                  stride = 1,\n",
    "                                  padding = (2, 0),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = nn.PReLU()\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv21(x)\n",
    "        x = self.conv22(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            if torch.cuda.is_available():\n",
    "                extras = extras.cuda()\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "        \n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UBNeck(nn.Module):\n",
    "    \n",
    "  # Upsampling bottleneck:\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # conv2d-1x1     convTrans2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-1x1\n",
    "  #      |             |\n",
    "  # maxunpool2d    Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  #  Params: \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  \n",
    "    def __init__(self, in_channels, out_channels, relu=False, projection_ratio=4):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n",
    "                                     stride = 2)\n",
    "        \n",
    "        self.main_conv = nn.Conv2d(in_channels = self.in_channels,\n",
    "                                    out_channels = self.out_channels,\n",
    "                                    kernel_size = 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        # This layer used for Upsampling\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = 2,\n",
    "                                  padding = 1,\n",
    "                                  output_padding = 1,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x, indices):\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.convt1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.convt2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.convt3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        x_copy = self.main_conv(x_copy)\n",
    "        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n",
    "        \n",
    "        # summing the main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENet(nn.Module):\n",
    "  \n",
    "  # Creating Enet model!\n",
    "  \n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        # The initial block\n",
    "        self.init = InitialBlock()\n",
    "        \n",
    "        \n",
    "        # The first bottleneck\n",
    "        self.b10 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=64, \n",
    "                           down_flag=True, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b11 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b12 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b13 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b14 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        \n",
    "        # The second bottleneck\n",
    "        self.b20 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=128, \n",
    "                           down_flag=True)\n",
    "        \n",
    "        self.b21 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b22 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b23 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b24 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b25 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b26 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b27 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b28 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The third bottleneck\n",
    "        self.b31 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b32 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b33 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b34 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b35 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b36 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b37 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b38 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        self.b40 = UBNeck(in_channels=128, \n",
    "                          out_channels=64, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b41 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        self.b42 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        self.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b51 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=16, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                           out_channels=self.C, \n",
    "                                           kernel_size=3, \n",
    "                                           stride=2, \n",
    "                                           padding=1, \n",
    "                                           output_padding=1,\n",
    "                                           bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The initial block\n",
    "        x = self.init(x)\n",
    "        \n",
    "        # The first bottleneck\n",
    "        x, i1 = self.b10(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.b14(x)\n",
    "        \n",
    "        # The second bottleneck\n",
    "        x, i2 = self.b20(x)\n",
    "        x = self.b21(x)\n",
    "        x = self.b22(x)\n",
    "        x = self.b23(x)\n",
    "        x = self.b24(x)\n",
    "        x = self.b25(x)\n",
    "        x = self.b26(x)\n",
    "        x = self.b27(x)\n",
    "        x = self.b28(x)\n",
    "        \n",
    "        # The third bottleneck\n",
    "        x = self.b31(x)\n",
    "        x = self.b32(x)\n",
    "        x = self.b33(x)\n",
    "        x = self.b34(x)\n",
    "        x = self.b35(x)\n",
    "        x = self.b36(x)\n",
    "        x = self.b37(x)\n",
    "        x = self.b38(x)\n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        x = self.b40(x, i2)\n",
    "        x = self.b41(x)\n",
    "        x = self.b42(x)\n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        x = self.b50(x, i1)\n",
    "        x = self.b51(x)\n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        x = self.fullconv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENetDecoder(nn.Module):\n",
    "    # Creating Branched Enet model!\n",
    "  \n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C    \n",
    "        \n",
    "         \n",
    "        # The third bottleneck\n",
    "        self.b31 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b32 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b33 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b34 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b35 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b36 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b37 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b38 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        self.b40 = UBNeck(in_channels=128, \n",
    "                          out_channels=64, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b41 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        self.b42 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        self.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b51 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=16, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                           out_channels=self.C, \n",
    "                                           kernel_size=3, \n",
    "                                           stride=2, \n",
    "                                           padding=1, \n",
    "                                           output_padding=1,\n",
    "                                           bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, i1, i2):\n",
    "        \n",
    "       \n",
    "        # The third bottleneck\n",
    "        x = self.b31(x)\n",
    "        x = self.b32(x)\n",
    "        x = self.b33(x)\n",
    "        x = self.b34(x)\n",
    "        x = self.b35(x)\n",
    "        x = self.b36(x)\n",
    "        x = self.b37(x)\n",
    "        x = self.b38(x)\n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        x = self.b40(x, i2)\n",
    "        x = self.b41(x)\n",
    "        x = self.b42(x)\n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        x = self.b50(x, i1)\n",
    "        x = self.b51(x)\n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        x = self.fullconv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchedModule(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super(BranchedModule, self).__init__()\n",
    "        self.C = C\n",
    "        self.layer1 = ENetDecoder(C)\n",
    "        self.layer2 = ENetDecoder(C)\n",
    "        self.layer3 = ENetDecoder(C)\n",
    "        \n",
    "    def forward(self,x, i1, i2):\n",
    "        y = [self.layer1(x, i1, i2), self.layer2(x, i1, i2), self.layer3(x, i1, i2)]\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchedENet(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        # The initial block\n",
    "        self.init = InitialBlock()\n",
    "        \n",
    "        \n",
    "        # The first bottleneck\n",
    "        self.b10 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=64, \n",
    "                           down_flag=True, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b11 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b12 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b13 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b14 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        \n",
    "        # The second bottleneck\n",
    "        self.b20 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=128, \n",
    "                           down_flag=True)\n",
    "        \n",
    "        self.b21 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b22 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b23 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b24 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b25 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b26 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b27 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b28 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.branch = BranchedModule(C)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The initial block\n",
    "        x = self.init(x)\n",
    "        \n",
    "        # The first bottleneck\n",
    "        x, i1 = self.b10(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.b14(x)\n",
    "        \n",
    "        # the second bottleneck\n",
    "        x, i2 = self.b20(x)\n",
    "        x = self.b21(x)\n",
    "        x = self.b22(x)\n",
    "        x = self.b23(x)\n",
    "        x = self.b24(x)\n",
    "        x = self.b25(x)\n",
    "        x = self.b26(x)\n",
    "        x = self.b27(x)\n",
    "        x = self.b28(x)\n",
    "        x = self.branch(x, i1, i2)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = torch.load('models/ckpt-enet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Losses(todo)\n",
    "(1) Semantic Segmentation Loss\n",
    "\n",
    "(2) Instantance Segmentation Loss\n",
    "\n",
    "(3) Depth Estimation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-19-1dd30558c734>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-1dd30558c734>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    local M = {}\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def compute_instance_cost(a_C, a_G):\n",
    "local M = {}\n",
    "local tools = require 'tools/tools'\n",
    "\n",
    "local in_margin = 0.5\n",
    "local out_margin = 1.5\n",
    "local Lnorm = 2\n",
    "\n",
    "local function norm(inp, L)\n",
    "    local n\n",
    "    if (L == 1) then\n",
    "        n = torch.sum(torch.abs(inp), 1)\n",
    "    else\n",
    "        n = torch.sqrt(torch.sum(torch.pow(inp, 2), 1) + 1e-8)\n",
    "    end\n",
    "    return n\n",
    "end\n",
    "\n",
    "-- prediction: batchsize x nDim x h x w\n",
    "-- labels: batchsize x classes x h x w\n",
    "\n",
    "local lossf =\n",
    "    function(prediction, labels)\n",
    "    local batchsize = prediction:size(1)\n",
    "    local c = prediction:size(2)\n",
    "    local height = prediction:size(3)\n",
    "    local width = prediction:size(4)\n",
    "    local nInstanceMaps = labels:size(2)\n",
    "    local loss = 0\n",
    "\n",
    "    M.loss_dist = 0\n",
    "    M.loss_var = 0\n",
    "\n",
    "    for b = 1, batchsize do\n",
    "        local pred = prediction[b] -- c x h x w\n",
    "        local loss_var = 0\n",
    "        local loss_dist = 0\n",
    "\n",
    "        for h = 1, nInstanceMaps do\n",
    "            local label = labels[b][h]:view(1, height, width) -- 1 x h x w\n",
    "            local means = {}\n",
    "            local loss_v = 0\n",
    "            local loss_d = 0\n",
    "\n",
    "            -- center pull force\n",
    "            for j = 1, label:max() do\n",
    "                local mask = label:eq(j)\n",
    "                local mask_sum = mask:sum()\n",
    "                if (mask_sum > 1) then\n",
    "                    local inst = pred[mask:expandAs(pred)]:view(c, -1, 1) -- c x -1 x 1\n",
    "\n",
    "                    -- Calculate mean of instance\n",
    "                    local mean = torch.mean(inst, 2) -- c x 1 x 1\n",
    "                    table.insert(means, mean)\n",
    "\n",
    "                    -- Calculate variance of instance\n",
    "                    local var = norm((inst - mean:expandAs(inst)), 2) -- 1 x -1 x 1\n",
    "                    var = torch.cmax(var - (in_margin), 0)\n",
    "                    local not_hinged = torch.sum(torch.gt(var, 0))\n",
    "\n",
    "                    var = torch.pow(var, 2)\n",
    "                    var = var:view(-1)\n",
    "\n",
    "                    var = torch.mean(var)\n",
    "                    loss_v = loss_v + var\n",
    "                end\n",
    "            end\n",
    "\n",
    "            loss_var = loss_var + loss_v\n",
    "\n",
    "            -- center push force\n",
    "            if (#means > 1) then\n",
    "                for j = 1, #means do\n",
    "                    local mean_A = means[j] -- c x 1 x 1\n",
    "                    for k = j + 1, #means do\n",
    "                        local mean_B = means[k] -- c x 1 x 1\n",
    "                        local d = norm(mean_A - mean_B, Lnorm) -- 1 x 1 x 1\n",
    "                        d = torch.pow(torch.cmax(-(d - 2 * out_margin), 0), 2)\n",
    "                        loss_d = loss_d + d[1][1][1]\n",
    "                    end\n",
    "                end\n",
    "\n",
    "                loss_dist = loss_dist + loss_d / ((#means - 1) + 1e-8)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        loss = loss + (loss_dist + loss_var)\n",
    "    end\n",
    "\n",
    "    loss = loss / batchsize + torch.sum(prediction) * 0\n",
    "\n",
    "    return loss\n",
    "end\n",
    "\n",
    "return lossf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_huber_loss(input_i, label):\n",
    "    local mask = label:gt(0)\n",
    "    local d = input_i[mask] - label[mask]\n",
    "    local ds = d:size(1)\n",
    "\n",
    "    local da = torch.abs(d)\n",
    "    local d2 = torch.pow(d, 2)\n",
    "\n",
    "    local th = 1 / 5 * torch.max(da)\n",
    "    local mask2 = torch.gt(da, th)\n",
    "    da[mask2] = (d2[mask2] + (th * th)) / (2 * th)\n",
    "\n",
    "    return 1 / ds * torch.sum(da)\n",
    "\n",
    "return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local grad = require 'autograd'\n",
    "\n",
    "def lossfunction(lossf_name, weights):\n",
    "    if (lossf_name == 'softmaxLoss') then\n",
    "        lossfunction = cudnn.SpatialCrossEntropyCriterion(weights)\n",
    "    elseif (lossf_name == 'huberLoss') then\n",
    "        lossfunction = grad.nn.AutoCriterion('depthLoss_huber')(require 'lossf/myHuberLoss')\n",
    "    elseif (lossf_name == 'instanceLoss') then\n",
    "        lossfunction = grad.nn.AutoCriterion('instance_loss')(require 'lossf/myInstanceLoss')\n",
    "    else\n",
    "        assert(false, 'Cannot load lossfunction ' .. opts.lossf)\n",
    "    end\n",
    "\n",
    "    return lossfunction\n",
    "end\n",
    "\n",
    "return getLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Instantiate the ENet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BranchedENet(\n",
      "  (init): InitialBlock(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(3, 13, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (prelu): PReLU(num_parameters=16)\n",
      "    (batchnorm): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b10): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.01, inplace=False)\n",
      "    (conv1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b11): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.01, inplace=False)\n",
      "    (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b12): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.01, inplace=False)\n",
      "    (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b13): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.01, inplace=False)\n",
      "    (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b14): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.01, inplace=False)\n",
      "    (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b20): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b21): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b22): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b23): ASNeck(\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv21): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)\n",
      "    (conv22): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b24): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b25): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b26): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b27): ASNeck(\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv21): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)\n",
      "    (conv22): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b28): RDDNeck(\n",
      "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "    (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu1): PReLU(num_parameters=1)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
      "    (prelu2): PReLU(num_parameters=1)\n",
      "    (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (prelu3): PReLU(num_parameters=1)\n",
      "    (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch): BranchedModule(\n",
      "    (layer1): ENetDecoder(\n",
      "      (b31): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b32): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b33): ASNeck(\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv21): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)\n",
      "        (conv22): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b34): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b35): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b36): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b37): ASNeck(\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv21): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)\n",
      "        (conv22): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b38): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b40): UBNeck(\n",
      "        (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "        (main_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (convt1): ConvTranspose2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (convt2): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (prelu2): ReLU()\n",
      "        (convt3): ConvTranspose2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b41): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b42): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b50): UBNeck(\n",
      "        (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "        (main_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (convt1): ConvTranspose2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (convt2): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (prelu2): ReLU()\n",
      "        (convt3): ConvTranspose2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b51): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fullconv): ConvTranspose2d(16, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (layer2): ENetDecoder(\n",
      "      (b31): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b32): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b33): ASNeck(\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv21): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)\n",
      "        (conv22): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b34): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b35): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b36): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b37): ASNeck(\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv21): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)\n",
      "        (conv22): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b38): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b40): UBNeck(\n",
      "        (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "        (main_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (convt1): ConvTranspose2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (convt2): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (prelu2): ReLU()\n",
      "        (convt3): ConvTranspose2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b41): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b42): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b50): UBNeck(\n",
      "        (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "        (main_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (convt1): ConvTranspose2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (convt2): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (prelu2): ReLU()\n",
      "        (convt3): ConvTranspose2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b51): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fullconv): ConvTranspose2d(16, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (layer3): ENetDecoder(\n",
      "      (b31): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b32): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b33): ASNeck(\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv21): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)\n",
      "        (conv22): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b34): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b35): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b36): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b37): ASNeck(\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv21): Conv2d(32, 32, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2), bias=False)\n",
      "        (conv22): Conv2d(32, 32, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), bias=False)\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b38): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): PReLU(num_parameters=1)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
      "        (prelu2): PReLU(num_parameters=1)\n",
      "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): PReLU(num_parameters=1)\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b40): UBNeck(\n",
      "        (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "        (main_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (convt1): ConvTranspose2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (convt2): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (prelu2): ReLU()\n",
      "        (convt3): ConvTranspose2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b41): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b42): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b50): UBNeck(\n",
      "        (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "        (main_conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (convt1): ConvTranspose2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (convt2): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "        (prelu2): ReLU()\n",
      "        (convt3): ConvTranspose2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (b51): RDDNeck(\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (dropout): Dropout2d(p=0.1, inplace=False)\n",
      "        (conv1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu1): ReLU()\n",
      "        (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (prelu2): ReLU()\n",
      "        (conv3): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (prelu3): ReLU()\n",
      "        (batchnorm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fullconv): ConvTranspose2d(16, 12, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "enet = BranchedENet(12)\n",
    "print(enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
    "device = torch.device('cuda:0')\n",
    "enet = enet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Define the loader that will load the input and output images(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(training_path, segmented_path, batch_size, h=320, w=1000):\n",
    "    filenames_t = os.listdir(training_path)\n",
    "    total_files_t = len(filenames_t)\n",
    "    \n",
    "    filenames_s = os.listdir(segmented_path)\n",
    "    total_files_s = len(filenames_s)\n",
    "    \n",
    "    assert(total_files_t == total_files_s)\n",
    "    \n",
    "    if str(batch_size).lower() == 'all':\n",
    "        batch_size = total_files_s\n",
    "    \n",
    "    idx = 0\n",
    "    while(1):\n",
    "      # Choosing random indexes of images and labels\n",
    "        batch_idxs = np.random.randint(0, total_files_s, batch_size)\n",
    "            \n",
    "        \n",
    "        inputs = []\n",
    "        labels = []\n",
    "        \n",
    "        for jj in batch_idxs:\n",
    "          # Reading normalized photo\n",
    "            img = plt.imread(training_path + filenames_t[jj])\n",
    "          # Resizing using nearest neighbor method\n",
    "            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n",
    "            inputs.append(img)\n",
    "          \n",
    "          # Reading semantic image\n",
    "            img = Image.open(segmented_path + filenames_s[jj])\n",
    "            img = np.array(img)\n",
    "          # Resizing using nearest neighbor method\n",
    "            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n",
    "            labels.append(img)\n",
    "         \n",
    "        inputs = np.stack(inputs, axis=2)\n",
    "      # Changing image format to C x H x W\n",
    "        inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n",
    "        \n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        yield inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Define the class weights(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(num_classes, c=1.02):\n",
    "    pipe = loader('content/train/', 'content/trainannot/', batch_size='all')\n",
    "    _, labels = next(pipe)\n",
    "    all_labels = labels.flatten()\n",
    "    print(all_labels)\n",
    "    each_class = np.bincount(all_labels, minlength=num_classes)\n",
    "    prospensity_score = each_class / len(all_labels)\n",
    "    class_weights = 1 / (np.log(c + prospensity_score))\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 11, 11,  ...,  3,  3,  3], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "class_weights = get_class_weights(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Define the Hyperparameters(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
    "optimizer = torch.optim.Adam(enet.parameters(), \n",
    "                             lr=lr,\n",
    "                             weight_decay=2e-4)\n",
    "\n",
    "print_every = 5\n",
    "eval_every = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Training loop(todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/36 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 3.15 GiB already allocated; 0 bytes free; 214.05 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4a0e987f1be6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# loss calculation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-b20e54527229>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb27\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb28\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbranch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-4d7057008059>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, i1, i2)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-b5276631f42e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, i1, i2)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# The fourth bottleneck\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb40\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb41\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb42\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-a5407195c4fd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, indices)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprelu1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvt2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprelu2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    785\u001b[0m         return F.conv_transpose2d(\n\u001b[0;32m    786\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 3.15 GiB already allocated; 0 bytes free; 214.05 MiB cached)"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "bc_train = 367 // batch_size # mini_batch train\n",
    "bc_eval = 101 // batch_size  # mini_batch validation\n",
    "\n",
    "# Define pipeline objects\n",
    "pipe = loader('content/train/', 'content/trainannot/', batch_size)\n",
    "eval_pipe = loader('content/val/', 'content/valannot/', batch_size)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Train loop\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    print ('-'*15,'Epoch %d' % e, '-'*15)\n",
    "    \n",
    "    enet.train()\n",
    "    \n",
    "    for _ in tqdm(range(bc_train)):\n",
    "        X_batch, mask_batch = next(pipe)\n",
    "        \n",
    "        # assign data to cpu/gpu\n",
    "        X_batch, mask_batch = X_batch.to(device), mask_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = enet(X_batch.float())\n",
    "        \n",
    "        # loss calculation\n",
    "        loss = criterion(out, mask_batch.long())\n",
    "        # update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        \n",
    "    print ()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if (e+1) % print_every == 0:\n",
    "        print ('Epoch {}/{}...'.format(e, epochs),\n",
    "                'Loss {:6f}'.format(train_loss))\n",
    "    \n",
    "    if e % eval_every == 0:\n",
    "        with torch.no_grad():\n",
    "            enet.eval()\n",
    "            \n",
    "            eval_loss = 0\n",
    "\n",
    "            # Validation loop\n",
    "            for _ in tqdm(range(bc_eval)):\n",
    "                inputs, labels = next(eval_pipe)\n",
    "\n",
    "                \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                \n",
    "                out = enet(inputs)\n",
    "                \n",
    "                out = out.data.max(1)[1]\n",
    "                \n",
    "                eval_loss += (labels.long() - out.long()).sum()\n",
    "                \n",
    "            \n",
    "            print ()\n",
    "            print ('Loss {:6f}'.format(eval_loss))\n",
    "            \n",
    "            eval_losses.append(eval_loss)\n",
    "        \n",
    "    if e % print_every == 0:\n",
    "        checkpoint = {\n",
    "            'epochs' : e,\n",
    "            'state_dict' : enet.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, '/content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
    "        print ('Model saved!')\n",
    "\n",
    "print ('Epoch {}/{}...'.format(e, epochs),\n",
    "       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
